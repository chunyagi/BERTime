{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chunyagi/ViTimeLLM/blob/main/timellm_paper_replicating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47RDbUeRIbZA"
      },
      "source": [
        "# Time-LLM Paper Replicating (MVP)\n",
        "\n",
        "In this notebook, we're going to replicate the TimeLLM architecture/paper with PyTorch: https://arxiv.org/abs/2310.01728\n",
        "\n",
        "To start off, we're first going to build a minimal end-to-end implementation to validate the core workflow. Once that's working, we'll progressively scale up to the full Time-LLM (Llama-7B) architecture as described in the paper.\n",
        "\n",
        "Workflow:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0QTINU6dR9h"
      },
      "source": [
        "## 0. Get setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htdoV6kCgFWj"
      },
      "outputs": [],
      "source": [
        "# Import libraries that are pre-installed in google colab\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "from typing import List, Tuple, Dict, Callable, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zASBXiLahNAZ"
      },
      "outputs": [],
      "source": [
        "# Install torchinfo\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo, installing it...\")\n",
        "  !pip3 install -q torchinfo\n",
        "  from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-xv99TEh8Yf"
      },
      "outputs": [],
      "source": [
        "# Setup device agnostic code\n",
        "def set_device():\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "  elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "    device = \"mps\"\n",
        "  else:\n",
        "    device = \"cpu\"\n",
        "  return device\n",
        "\n",
        "DEVICE = set_device()\n",
        "print(f\"[INFO] Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIJjTQSKjRaz"
      },
      "source": [
        "## 1. Get data\n",
        "\n",
        "For simplicity, we're going to create a toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYl4Ks_zlJxs"
      },
      "outputs": [],
      "source": [
        "# For completeness\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Setup periods\n",
        "PERIODS = 24 * 7 # one day of data\n",
        "\n",
        "# Create datatime index\n",
        "dates = pd.date_range(\"2025-01-01\", periods=PERIODS, freq=\"h\")\n",
        "\n",
        "# Create sine and cos curves as toy data\n",
        "sin_data = np.sin(np.linspace(0, 2 * np.pi, PERIODS))\n",
        "cos_data = np.cos(np.linspace(0, 2 * np.pi, PERIODS))\n",
        "\n",
        "# Create toy dataset\n",
        "toy_df = pd.DataFrame(\n",
        "    {\"sin_data\": sin_data,\n",
        "     \"cos_data\": cos_data},\n",
        "    index=dates\n",
        ")\n",
        "toy_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8bGgqPdJlAk"
      },
      "outputs": [],
      "source": [
        "len(toy_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyCfZ3sfuujZ"
      },
      "outputs": [],
      "source": [
        "# Visualise data\n",
        "toy_df.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ0V_qSNuv6I"
      },
      "source": [
        "## 2. Create Datasets and DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSYpHrlAMzIZ"
      },
      "source": [
        "### 2.1 Split the data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am81c6eRv9jr"
      },
      "outputs": [],
      "source": [
        "# Let's split the data into training and test sets\n",
        "train_split = int(len(toy_df) * 0.8)\n",
        "train_df = toy_df[:train_split]\n",
        "test_df = toy_df[train_split:]\n",
        "print(f\"[INFO] Size of the training data: {len(train_df)}\")\n",
        "print(f\"[INFO] Size of the test data: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlah_vwWKMpd"
      },
      "source": [
        "### 2.2 Create a custom PyTorch `Dataset` to replicate `TimeSeriesDataset` from PyTorch Forecasting\n",
        "\n",
        "To create our own custom dataset, we want to:\n",
        "1. Subclass `torch.utils.data.Dataset`.\n",
        "2. Init our subclass with our target dataframe, (sliding) window size, forecast horizon, stride as well as a transform if we'd like to apply global transform (e.g. standardization) on our data (not necessary here).\n",
        "3. Create several attributes:\n",
        "* `df` - target dataset\n",
        "* `window_size` - size of the sliding window, also called the history length (number of data points in the past the model uses to predict the future values)\n",
        "* `forecast_horizon` - number of steps ahead the model wants to predict in the future\n",
        "* `stride` - step size for sliding the input window across the time series\n",
        "* `num_samples` - number of training samples\n",
        "* `transform` - the (global) transform we'd like to use (not necessary here, just for completeness)\n",
        "4. Overwrite the `__len__()` method to return the length of our dataset\n",
        "5. Overwrite the `__getitem__()` method to return a given sample pair (in the form of a `Tuple`, `(input_window, target)`) when passed an index.\n",
        "\n",
        "**Note:**\n",
        "- Window size $\\neq$ patch size! One input window could correspond to more than one patch!\n",
        "- The `stride` here is not the same as the stride used in the time series patchification later!\n",
        "- Given an index, want the `__getitem__()` method to return a tuple `(input_window, target)`\n",
        " - `input_window` is a tensor of shape `(number_of_channels, window_size)`\n",
        " - `target` is a tensor of shape `(number_of_channels, forcast_horizon)`, where `num_of_channels` means how many time series there are in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z695D4IPNeHW"
      },
      "outputs": [],
      "source": [
        "# Write a custom dataset class and do the import\n",
        "from torch.utils.data import Dataset\n",
        "from typing import List, Tuple, Dict, Callable, Optional\n",
        "\n",
        "# Subclass torch.utils.data.Dataset\n",
        "class TimeSeriesDatasetCustom(Dataset):\n",
        "  # Initialize our custom dataset\n",
        "  def __init__(self,\n",
        "               df: pd.DataFrame,\n",
        "               window_size: int,\n",
        "               forecast_horizon: int = 1,\n",
        "               stride: int = 1,\n",
        "               transform: Optional[Callable[[torch.tensor], torch.tensor]] = None): # not necessary, just for completeness\n",
        "    super().__init__() # optinal here, but doesn't hurt\n",
        "\n",
        "    # Create class attributes\n",
        "    self.df = df\n",
        "    self.window_size = window_size\n",
        "    self.forecast_horizon = forecast_horizon\n",
        "    self.stride = stride\n",
        "    self.transform = transform\n",
        "\n",
        "  # Overwrite __len__()\n",
        "  def __len__(self) -> int:\n",
        "    \"Returns the total number of training samples.\"\n",
        "    # Calcuate number of training samples\n",
        "    num_samples = (len(self.df) - self.window_size - self.forecast_horizon) // self.stride + 1\n",
        "    return num_samples\n",
        "\n",
        "  # Overwrite __getitem__() method to return a particular sample pair (input_window, target)\n",
        "  def __getitem__(self, index: int) -> Tuple[torch.tensor, torch.tensor]:\n",
        "    \"Returns one sample of data, data and label (x, y).\"\n",
        "    # Normalise negative indicies\n",
        "    if index < 0:\n",
        "      index = len(self) + index\n",
        "\n",
        "    start = index * self.stride\n",
        "    x = self.df[start: start + self.window_size].to_numpy()\n",
        "    y = self.df[start + self.window_size: start + self.window_size + self.forecast_horizon].to_numpy()\n",
        "\n",
        "    # Convert X & y to tensors\n",
        "    x = torch.from_numpy(x).type(torch.float32) # convert the datatype from float64 (Numpy's default dtype) to float32 (PyTorch's default dtype)\n",
        "    y = torch.from_numpy(y).type(torch.float32)\n",
        "\n",
        "    # Change the output shape of x (pytorch prefers channel-first)\n",
        "    if x.ndim == 1: # univariate time series data\n",
        "      x = x.unsqueeze(dim=0) # add channel size\n",
        "    else: # multivariate time series data\n",
        "      x = x.transpose(0, 1) # channel first\n",
        "\n",
        "    # Change the output shape of y as well\n",
        "    if y.ndim == 1:\n",
        "      y = y.unsqueeze(dim=0) # add channel size\n",
        "    else:\n",
        "      y = y.transpose(0, 1) # channel first\n",
        "\n",
        "    # Transform if necessary\n",
        "    if self.transform:\n",
        "      return self.transform(x), y\n",
        "    else:\n",
        "      return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiVuNPojTyoN"
      },
      "outputs": [],
      "source": [
        "toy_df[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42btCopr65EJ"
      },
      "outputs": [],
      "source": [
        "# Test out TimeSeriesDatasetCustom\n",
        "train_data_custom = TimeSeriesDatasetCustom(df=train_df,\n",
        "                                            window_size=6,\n",
        "                                            stride=5) # non-overlapping window because it is equal to the window size\n",
        "test_data_custom = TimeSeriesDatasetCustom(df=test_df,\n",
        "                                           window_size=6,\n",
        "                                           stride=5)\n",
        "train_data_custom, test_data_custom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2x6Lfv3FppG"
      },
      "outputs": [],
      "source": [
        "# Test out the __len__() method\n",
        "# Print out some info about our dataset\n",
        "print(f\"[INFO] Size of the training dataset: {len(train_data_custom)}\")\n",
        "print(f\"[INFO] Size of the test dataset: {len(test_data_custom)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKrXMABcHj61"
      },
      "outputs": [],
      "source": [
        "# Verify the number of samples (size of the dataset - window_size)\n",
        "len(train_data_custom) == (len(train_df) - 5 - 1) // 5 + 1, len(test_data_custom) == (len(test_df) - 5 - 1) // 5 + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-4xlWxfJhGT"
      },
      "outputs": [],
      "source": [
        "# Print some more info to test out the __getitem__() method\n",
        "dataset_dict = {\"training\": train_data_custom,\n",
        "                \"test\": test_data_custom}\n",
        "\n",
        "for split, dataset in dataset_dict.items():\n",
        "  print(f\"[INFO] Sample 0 of {split} set:\")\n",
        "  print(f\"Data:\\n{dataset[0][0]}\")\n",
        "  print(f\"Shape: {dataset[0][0].shape}\")\n",
        "  print()\n",
        "  print(f\"Target:\\n{dataset[0][1]}\")\n",
        "  print(f\"Shape: {dataset[0][1].shape}\")\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD_ZzkIJLipz"
      },
      "source": [
        "### 2.2 Turn custom loaded time series data into DataLoader's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "472Hq2uwMP-7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup number of workers and batch size\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "BATCH_SIZE = 3\n",
        "\n",
        "# Turn the custom pytorch Datasets into DataLoaders\n",
        "train_dataloader_custom = DataLoader(dataset=train_data_custom,\n",
        "                                     batch_size=BATCH_SIZE,\n",
        "                                     num_workers=NUM_WORKERS,\n",
        "                                     shuffle=True,\n",
        "                                     drop_last=False)\n",
        "test_dataloader_custom = DataLoader(dataset=test_data_custom,\n",
        "                                    batch_size=6,\n",
        "                                    num_workers=NUM_WORKERS,\n",
        "                                    shuffle=False, # no need to shuffle the test data\n",
        "                                    drop_last=False)\n",
        "train_dataloader_custom, test_dataloader_custom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANkSedMDwyh8"
      },
      "outputs": [],
      "source": [
        "# Print out some information of the custom dataloaders\n",
        "print(f\"[INFO] Train dataloader (custom) has: {len(train_dataloader_custom)} batches with number of samples: {BATCH_SIZE}\")\n",
        "print(f\"[INFO] Test dataloader (custom) has: {len(test_dataloader_custom)} batches with number of samples: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nuNofVKxnMh"
      },
      "outputs": [],
      "source": [
        "# Get a sample from the custom train dataloader\n",
        "input_window, target = next(iter(train_dataloader_custom))\n",
        "input_window.shape, target.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vGCjupNx8us"
      },
      "source": [
        "## 3. Replicating TimeLLM: Overview\n",
        "\n",
        "With the dataloader's ready, we can now proceed to replicate TimeLLM's model architecture. Before we start, it's a good practice to think about the **inputs** and **outputs**.\n",
        "\n",
        "* Inputs - What goes into the model? A pair consisting of:\n",
        "  * Input window: A segment of the time series with length `window_size`\n",
        "    * Shape: `(batch_size, num_channels, window_size)`\n",
        "    * E.g. `(32, 2, 48)` - 32 samples in a batch, 2 time series, window size of 48\n",
        "  * Target values: The ground truth value we want our model to predict\n",
        "    * Shape: `(batch_size, num_channels, forecast_horizon)`\n",
        "    * E.g. `(32, 2, 24)` - 32 samples in a batch, 2 time series, 24 ground truth future values\n",
        "\n",
        "* Outputs - What comes out of the model?\n",
        "  * Predictions for future time steps\n",
        "    * Shape: `(batch_size, forecast_horizon)`\n",
        "    * E.g. `(32, 24)` - 32 samples in a batch, 24 predictions into the future\n",
        "  \n",
        "> Note: The shape of the model outpus doesn't have `num_channels` in the second dimension because our model processes each (univariate) time series subsequently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGh3W4gUPJyT"
      },
      "source": [
        "### 3.1 TimeLLM overview: pieces of the puzzle\n",
        "\n",
        "* Section 3 introduction + Figure 2: Paper objectives & model architecture overview\n",
        "* Four components: The main four components which TimeLLM can be broken down into\n",
        "\n",
        "#### The introduction of Section 3 outlines the objectives of this paper and provides an overview of the model architecture. Here are some snippets:\n",
        "\n",
        "* Objective:\n",
        "Our model architecture is depicted in Fig. 2. We focus on reprogramming an embedding-visible language foundation model, such as Llama (Touvron et al., 2023) and GPT-2 (Radford et al., 2019), for general time series forecasting without requiring any fine-tuning of the backbone model. Specifically, we consider the following problem: given a sequence of historical observations $\\mathbf{X} \\in \\mathbb{R}^{N \\times T}$ consisting of $N$ different 1-dimensional variables across $T$ time steps, we aim to reprogram a large language model $f(\\cdot)$ to understand the input time series and accurately forecast the readings at $H$ future time steps, denoted by $\\hat{\\mathbf{Y}} \\in \\mathbb{R}^{N \\times H}$, with the overall objective to minimize the mean squared error between the ground truths $\\mathbf{Y}$ and predictions:\n",
        "$$\n",
        "\\frac{1}{H} \\sum_{h=1}^H \\| \\hat{Y}_h - Y_h \\|_F^2\n",
        "$$\n",
        "\n",
        "* Model architecture overview:\n",
        "Our method encompasses three main components:\n",
        "  1. input transformation\n",
        "  2. a pre-trained and frozen LLM\n",
        "  3. output projection.\n",
        "  \n",
        "  Initially, a multivariate time series is partitioned into $N$ univariate time series, which are subsequently processed independently (Nie et al., 2023). The $i$-th series is denoted as $\\mathbf{X}^{(i)} \\in \\mathbb{R}^{1 \\times T}$ which undergoes normalization, patching, and embedding prior to being reprogrammed with learned text prototypes to align the source and target modalities. Then, we augment the LLM’s time series reasoning ability by prompting it together with reprogrammed patches to generate output representations, which are projected to the final forecasts $\\hat{\\mathbf{Y}}^{(i)} \\in \\mathbb{R}^{1 \\times H}$.\n",
        "\n",
        "#### Figure 2\n",
        "\n",
        "  <img src=\"https://raw.githubusercontent.com/chunyagi/ViTimeLLM/main/figures/timellm_architecture.png\" width=600 alt=\"figure 2 from timellm paper\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I2eRJ7m3-Xy"
      },
      "source": [
        "### Four components\n",
        "\n",
        "#### Section 3.1 breaks TimeLLM into four components:\n",
        "\n",
        "1. **Input Embedding:**\n",
        "Each input channel $\\mathbf{X}^{(i)}$ is first individually normalized to have zero mean and unit standard deviation via reversible instance normalization (RevIN) in mitigating the time series distribution shift (Kim et al., 2021). Then, we divide $\\mathbf{X}^{(i)}$ into several consecutive overlapped or non-overlapped patches (Nie et al., 2023) with length $L_p$; thus the total number of input patches\n",
        "is $P = \\lfloor (T − L_p ) \\rfloor + 2$, where $S$ denotes the horizontal sliding stride. The underlying motivations $S$\n",
        "are two-fold: (1) better preserving local semantic information by aggregating local information into each patch and (2) serving as tokenization to form a compact sequence of input tokens, reducing\n",
        "computational burdens. Given these patches $\\mathbf{X}_P^{(i)} \\in \\mathbb{R}^{P \\times L_p}$ , we embed them as $\\mathbf{X}_P^{(i)} \\in \\mathbb{R}^{P \\times d_m}$\n",
        "adopting a simple linear layer as the patch embedder to create dimensions $d_m$.\n",
        "\n",
        "In summary, it can be broken down into 3 steps:\n",
        "1. Normalization: zero mean and unit std\n",
        "2. Patchification: patchify the input into $P$ patches, each of length $L_p$\n",
        "3. Embedding: embed the patches by a simple a linear layer to change their dimensions from $L_p$ to $d_m$\n",
        "\n",
        "**Note:** There is a hidden step that is not mentioned above. Before normalization, we need to slide an input window across the time series to create input segments.\n",
        "\n",
        "```python\n",
        "# Input Embedding\n",
        "# 1. Normalisation\n",
        "print(x_input.shape) # (batch_size, num_channels, window_size)\n",
        "mean = x_input.mean(dim=2, keepdim=True)\n",
        "std = x_input.std(dim=2, keepdim=True)\n",
        "x_input_normalized = (x_input - mean) / std\n",
        "\n",
        "# 2./3. Patchification + Embedding\n",
        "# We will use a Conv1d layer to patchify and embed the patches simultaneously\n",
        "patcher = Conv1d(in_channels=num_channels,\n",
        "out_channel=embedding_dim,\n",
        "kernel_size=patch_size,\n",
        "stride=patch_size, # non-overlapping patches\n",
        "padding=0) # no padding\n",
        "\n",
        "# Perform the forward pass\n",
        "x_input_patched = patcher(x_input_normalized)\n",
        "print(x_input_patched.shape) # (batch_size, embedding_dim, num_patches)\n",
        "# Make sure the returned sequence embedding dimensions are in the right order\n",
        "x_input_patched = x_input_patched.permute(0, 2, 1) # (batch_size, num_patches, embedding_dim)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMhf45HmPONS"
      },
      "source": [
        "2. **Patch Reprogramming**: To close this gap, we propose reprogramming $\\hat{\\mathbf{X}}_{P}^{(i)}$ using pre-trained word embeddings $\\mathbf{E} \\in \\mathbb{R}^{V \\times D}$ in the backbone, where $V$ is the vocabulary size. Nevertheless, there is no prior knowledge indicating which source tokens are directly relevant. Thus, simply leveraging $\\mathbf{E}$ will result in large and potentially dense reprogramming space. A simple solution is to maintain a small collection of text prototypes by linearly probing $\\mathbf{E}$, denoted as $\\mathbf{E}^{\\prime} \\in \\mathbb{R}^{V^{\\prime} \\times D}$, where $V^{\\prime} \\ll V$. An illustration is in Fig. 3(a). Text prototypes learn connecting language cues, e.g., \"short up\" (red lines) and \"steady down\" (blue lines), which are then combined to represent the local patch information (e.g., \"short up then down steadily\" for characterizing patch 5) without leaving the space where the language model is pre-trained. This approach is efficient and allows for the adaptive selection of relevant source information. To realize this, we employ a multi-head crossattention layer. Specifically, for each head $k=\\{1, \\cdots, K\\}$, we define query matrices $\\mathbf{Q}_{k}^{(i)}=\\hat{\\mathbf{X}}_{P}^{(i)} \\mathbf{W}_{k}^{Q}$, key matrices $\\mathbf{K}_{k}^{(i)}=\\mathbf{E}^{\\prime} \\mathbf{W}_{k}^{K}$, and value matrices $\\mathbf{V}_{k}^{(i)}=\\mathbf{E}^{\\prime} \\mathbf{W}_{k}^{V}$, where $\\mathbf{W}_{k}^{Q} \\in \\mathbb{R}^{d_{m} \\times d}$ and $\\mathbf{W}_{k}^{K}, \\mathbf{W}_{k}^{V} \\in \\mathbb{R}^{D \\times d}$. Specifically, $D$ is the hidden dimension of the backbone model, and $d=\\left\\lfloor\\frac{d_{m}}{K}\\right\\rfloor$. Then, we have the operation to reprogram time series patches in each attention head defined as:\n",
        "$$\n",
        "\\mathbf{Z}_{k}^{(i)}=\\operatorname{ATTENTION}\\left(\\mathbf{Q}_{k}^{(i)}, \\mathbf{K}_{k}^{(i)}, \\mathbf{V}_{k}^{(i)}\\right)=\\operatorname{SOFTMAX}\\left(\\frac{\\mathbf{Q}_{k}^{(i)} \\mathbf{K}_{k}^{(i) \\top}}{\\sqrt{d_{k}}}\\right) \\mathbf{V}_{k}^{(i)}\n",
        "$$\n",
        "\n",
        "  By aggregating each $\\mathbf{Z}_{k}^{(i)} \\in \\mathbb{R}^{P \\times d}$ in every head, we obtain $\\mathbf{Z}^{(i)} \\in \\mathbb{R}^{P \\times d_{m}}$. This is then linearly projected to align the hidden dimensions with the backbone model, yielding $\\mathbf{O}^{(i)} \\in \\mathbb{R}^{P \\times D}$.\n",
        "\n",
        "#### Figure 3(a)\n",
        "\n",
        "  <img src=\"\" width=600 alt=\"figure 3a from timellm paper\">\n",
        "\n",
        "In summary, it can be broken down into 3 steps:\n",
        "1. Create text prototypes $\\mathbf{E'} \\in \\mathbb{R}^{V' \\times D} $ by running $\\mathbf{E} \\in \\mathbb{R}^{V \\times D}$ (the word embeddings in the backbone model) through a linear layer\n",
        "2. Employ a multi-head attention crossattention layer on the time series patches (queries) and text prototypes (keys and values) to align the source and target modalities\n",
        "3. Run the reprogrammed patches $\\mathbf{Z}^{(i)} \\in \\mathbb{R}^{P \\times D}$ through a linear layer to align with the backbone's hidden dimensions $D$.\n",
        "\n",
        "In pseudocode:\n",
        "```python\n",
        "# 1. Create text prototypes\n",
        "linear_layer_for_text_prototypes = nn.Linear(in_features=vocab_size, out_features=text_prototype_size)\n",
        "\n",
        "text_prototypes = linear_layer_for_text_prototypes(llm_word_embeddings)\n",
        "\n",
        "# 2. Reprogramming time series patches with multi-head cross attention with text prototypes\n",
        "reprogrammed_patch_embeddings = multihead_cross_attention(query=patches, key=llm_embeddings, value=llm_embeddings)\n",
        "\n",
        "# 3. Run the reprogrammed patches through a linear layer to align with the llm's hidden dimensions\n",
        "linear_layer_for_llm_hidden_dim = nn.Linear(in_features=patch_embedding_dim, out_features=llm_hidden_dim)\n",
        "reprogrammed_patch_embeddings = linear_layer_for_llm_hidden_dim(reprogrammed_patch)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oNAwhTLg-64"
      },
      "source": [
        "3. **Prompt-as-Prefix:** Prompts act as prefixes to enrich the input context and guide the transformation of reprogrammed time series patches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7X_pjdJguWh"
      },
      "source": [
        "4. **Output Projection:** Upon packing and feedforwarding the prompt and patch embeddings $\\mathbf{O}^{(i)}$ through the frozen LLM as shown in Fig. 2, we discard the prefixal part and obtain the output representations. Following this, we flatten and linear project them to derive the final forecasts $\\hat{\\mathbf{Y}}^{(i)}$.\n",
        "\n",
        "In pseudocode:\n",
        "```python\n",
        "# Input of the backbone model\n",
        "prefix = [token_0, token_1, ...]\n",
        "reprogammed_patch_embeddings = [patch_0, patch_1, ...]\n",
        "input_embeddings = prefix + reprogrammed_patch_embeddings\n",
        "\n",
        "# Perform forward pass to the LLM\n",
        "output_embeddings = LLM(input_embeddings)\n",
        "\n",
        "# Drop the (contextualised) prefix embeddings\n",
        "output_embeddings = output_embeddings[:, prefix_length:, :] # (batch_size, num_patches, embedding_dim)\n",
        "\n",
        "# flatten and pass them to the final forecast layer\n",
        "output_embeddings_flattened = torch.flatten(output_embeddings, start_dim=1) # (batch_size, num_patches * embedding_dim)\n",
        "\n",
        "# Create the forecast head\n",
        "forecast_layer = nn.Linear(in_features=num_patches*embedding_dim,\n",
        "out_features=forecast_horizen) # number of steps ahead (H)\n",
        "\n",
        "# Pass the embedding to the forecast layer\n",
        "forecast = forecast_layer(out_embeddings_flattened)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nin7y5kWGbMO"
      },
      "source": [
        "## 4. Component 1: Normalise the input windows, patchify them and turn them into patch embeddings\n",
        "\n",
        "Let's remind ourselves of the steps in component 1:\n",
        "\n",
        "> In summary, it can be broken down into 3 steps:\n",
        "1. Normalization: zero mean and unit std\n",
        "2. Patchification: patchify the input into $P$ patches, each of length $L_p$\n",
        "3. Embedding: embed the patches by a simple a linear layer to change their dimensions from $L_p$ to $d_m$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R-sgSjDXEyn"
      },
      "source": [
        "### 4.1 Normalise the input windows (RevIN)\n",
        "\n",
        "Before we jump into turning the patch embedding layer into a PyTorch module, we need to create a PyTorch module for RevIN (normalisation/inverse-normalisation).\n",
        "\n",
        "Note: Not only will RevIN normalises the input time series segments (to zero mean and unit variance), but it also allows additional (per channel) scaling and shifting:\n",
        "> $x_\\mathrm{norm} = \\gamma \\cdot\n",
        "(\\frac{x-\\mu}{\\sigma+\\epsilon}) + \\beta$\n",
        "\n",
        "Let's try to set up an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBwGMGtNXhe2"
      },
      "outputs": [],
      "source": [
        "# Create example values\n",
        "batch_size = 2\n",
        "num_channels = 2\n",
        "window_size = 6\n",
        "\n",
        "# Create an example batch of input windows\n",
        "torch.manual_seed(42)\n",
        "example_windows = torch.randint(10, size=(batch_size, num_channels, window_size)).type(torch.float32) # (batch size, num_channels, window_size)\n",
        "print(f\"Example input time window:\\n{example_windows}\\n\")\n",
        "\n",
        "# Calculate mean and std\n",
        "mean = torch.mean(example_windows, dim=2, keepdim=True)\n",
        "std = torch.std(example_windows, dim=2, keepdim=True)\n",
        "print(f\"Example mean:\\n{mean}\\n\")\n",
        "print(f\"Example standard deviation:\\n{std}\")\n",
        "print(f\"Mean/std shape: {mean.shape}\\n\")\n",
        "\n",
        "# Create gamma and beta\n",
        "gamma = torch.randint(10, size=(1, num_channels, 1)) # batch size is set to 1 for broadcasting\n",
        "beta = torch.randint(10, size=(1, num_channels, 1))\n",
        "print(f\"Gamma:\\n{gamma}\")\n",
        "print(f\"Beta:\\n{beta}\\n\")\n",
        "\n",
        "# Normalise the examples\n",
        "normalised_example_windows = (example_windows - mean) / std\n",
        "print(f\"Normalised example input time widow:\\n{normalised_example_windows}\")\n",
        "affine_normalised_example_windows = normalised_example_windows * gamma + beta\n",
        "print(f\"Affine normalised example input window:\\n{affine_normalised_example_windows}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CuiSBNnlfm9"
      },
      "source": [
        "### 4.2 Turn RevIN into a PyTorch module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESsJrVTpisZL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class RevIN(nn.Module):\n",
        "  \"\"\"Reversible instance normalisation (\"RevIN\" for short) for time series segments.\n",
        "\n",
        "  Args:\n",
        "    num_channels (int): Number of channels/time series in the dataset.\n",
        "    affine (bool): If True, apply a learnable per-channel scale and bias after normalisation.\n",
        "    eps (float): A small constant added to the denominator for numerical stability (prevents division by zero).\n",
        "\n",
        "  Returns:\n",
        "    (torch.tensor): Normalised (and linear transformed) input time series segments.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               num_channels: int,\n",
        "               affine: bool = False, # whether you want an affine transformation after the standard normalisation or not\n",
        "               eps: float = 1e-5): # added to the denominator to prevent division by zero error\n",
        "    super().__init__()\n",
        "\n",
        "    self.affine = affine\n",
        "    self.eps = eps\n",
        "\n",
        "    if affine:\n",
        "      self.gamma = nn.Parameter(torch.ones(1, num_channels, 1),\n",
        "                                requires_grad=True) # make sure it's learnable\n",
        "      self.beta = nn.Parameter(torch.zeros(1, num_channels, 1),\n",
        "                               requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Calculate the mean and variance of the input segments\n",
        "    self.mean = torch.mean(x, dim=2, keepdim=True) # set keepdim to True to keep the second dimension\n",
        "    self.std = torch.std(x, dim=2, keepdim=True)\n",
        "\n",
        "    # Normalise it to have zero mean and unit variance\n",
        "    x_norm = (x - self.mean) / (self.std + self.eps)\n",
        "\n",
        "    # If affine is enabled, learn a linear transformation as well\n",
        "    if self.affine:\n",
        "      x_norm = self.gamma * x_norm + self.beta\n",
        "\n",
        "    return x_norm\n",
        "\n",
        "  # Inverse normalisation to convert the input segments back to their original scale\n",
        "  def inverse(self, x_norm):\n",
        "    if self.affine:\n",
        "      # Reverse the linear transformation process\n",
        "      x_norm = (x_norm - self.beta) / (self.gamma + self.eps)\n",
        "\n",
        "    # Reverse normalisation\n",
        "    x = x_norm * self.std + self.mean\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSoUSK1rjVdp"
      },
      "outputs": [],
      "source": [
        "example_windows.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2GSfaffh22Y"
      },
      "outputs": [],
      "source": [
        "# Test the RevIN class\n",
        "# Create an instance of RevIN layer\n",
        "revin_layer = RevIN(num_channels=2,\n",
        "                    affine=True)\n",
        "\n",
        "# Pass example windows through RevIN layer\n",
        "print(f\"Input time series windows:\\n{example_windows}\\n\") # add an extra batch dimension\n",
        "revin_example_windows = revin_layer(example_windows)\n",
        "print(f\"Output normalised time series windows:\\n{revin_example_windows}\\n\")\n",
        "inverse_revin_example_windows = revin_layer.inverse(revin_example_windows)\n",
        "print(f\"Output inverse normalised time series windows:\\n{inverse_revin_example_windows}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CRrgRP_meQe"
      },
      "source": [
        "### 4.3 Calculate input and output shapes of the patch embedding layer by hand\n",
        "\n",
        "Batch size is omitted for simplicity here\n",
        "\n",
        "* Input shape: $N \\times W$ (number of channels/time series x window size)\n",
        "* Output shape: $P \\times d_m$ (number of patches x hidden dimension of the patch embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGhU-6uAWg9C"
      },
      "outputs": [],
      "source": [
        "# Create example values\n",
        "input_window_size = 6 # window size (W)\n",
        "num_of_channels = 2 # number of time series in the dataset (N)\n",
        "patch_size = 2 # patch length (L_p)\n",
        "non_overlap_stride = 2 # produce non-overlapping patches\n",
        "overlap_stride = 1 # produce overlapping patches\n",
        "patch_embedding_dim = 768 # patch embeddings dimension (d_m)\n",
        "\n",
        "# Calculate the number of patches (P) with non-overlapping stride (assumed no padding)\n",
        "number_of_non_overlap_patches = (input_window_size - patch_size) // non_overlap_stride + 1\n",
        "print(f\"Number of patches with non-overlapping stride {non_overlap_stride}: {number_of_non_overlap_patches}\")\n",
        "\n",
        "# Calculate the number of pacthes (P) with overlapping stride\n",
        "number_of_overlap_patches = (input_window_size - patch_size) // overlap_stride + 1\n",
        "print(f\"Number of patches with overlapping stride {overlap_stride}: {number_of_overlap_patches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12tuwpIJs9m1"
      },
      "outputs": [],
      "source": [
        "# Input shape\n",
        "patch_embedding_layer_input_shape = (num_of_channels, input_window_size)\n",
        "\n",
        "# Output shape (assumed non-overlapping stride)\n",
        "patch_embedding_layer_output_shape = (number_of_non_overlap_patches, patch_embedding_dim)\n",
        "\n",
        "print(f\"Input shape ({num_of_channels} input windows): {patch_embedding_layer_input_shape}\")\n",
        "print(f\"Output shape (1 sequence of non-overlapping patches): {patch_embedding_layer_output_shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMRUDw7xwQN_"
      },
      "source": [
        "### 4.4  Creating time series patches and turning them into patch embeddings\n",
        "\n",
        "> Note:\n",
        "The embedding method used in this implementation is probably different from the one stated in the paper. In the paper, a multivariate time series is first **partitioned into N univariate time series, which are subsequently processed independently.** But here we used a 1D convolutional layer to embed the information across all the time series **at once** into the patch embeddings. This may require a higher hidden dimension ($d_m$) to capture to capture more information from the other time series (therefore higher compute cost), but it streamlines the data flow pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkndoV9rsF2g"
      },
      "outputs": [],
      "source": [
        "# Create a conv1d layer to turn the time series input window into patches of learnable embeddings (patchification and embedding at the same time)\n",
        "from torch import nn\n",
        "\n",
        "# Set up the patch size (L_p)\n",
        "patch_size = 2\n",
        "\n",
        "conv1d = nn.Conv1d(in_channels=2, # number of time series\n",
        "                   out_channels=768, # patch embedding dimension\n",
        "                   kernel_size=patch_size,\n",
        "                   stride=patch_size, # non-overlapping patches\n",
        "                   padding=0) # no padding\n",
        "conv1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14HL4J1wuCsW"
      },
      "outputs": [],
      "source": [
        "# Get a training sample\n",
        "input_window = train_data_custom[0][0]\n",
        "\n",
        "# Remind ourselves of what the data looks like\n",
        "print(f\"[INFO] Input data:\\n{input_window}\") # take the first batch\n",
        "print(f\"[INFO] Shape: {input_window.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w-YXVjATyRs"
      },
      "outputs": [],
      "source": [
        "# Pass the input window through the convolutional layer\n",
        "input_window_out_of_conv = conv1d(input_window.unsqueeze(dim=0)) # add batch dimension\n",
        "input_window_out_of_conv, input_window_out_of_conv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OverHjUNWKpS"
      },
      "outputs": [],
      "source": [
        "input_window_out_of_conv.requires_grad # make sure it's learnable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYml2m96Y8IY"
      },
      "source": [
        "Let's interpret the shape of the output tensor. 1 is the batch size, 768 means, for each patch in the 1x3 grid, it has its own embeddings of 768 dimensions.\n",
        "\n",
        "Let's plot some random dimensions of the patch embeddings and see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhtgTa8iWunG"
      },
      "outputs": [],
      "source": [
        "# Plot random convolutional feature maps (embeddings)\n",
        "import random\n",
        "random_indexes = random.sample(range(0, 768), k=5)\n",
        "print(f\"Showing random convolutional feature maps from indexes {random_indexes}\")\n",
        "\n",
        "# Create plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(7, 7))\n",
        "\n",
        "# Plot random input window feature maps\n",
        "for i, idx in enumerate(random_indexes):\n",
        "  input_window_feature_map = input_window_out_of_conv[:, idx, :]\n",
        "  axs[i].imshow(input_window_feature_map.detach().numpy()) # remove gradient tracking and convert it to numpy array\n",
        "  axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6RRNrgsY3ix"
      },
      "outputs": [],
      "source": [
        "# Get a single feature map in tensor form\n",
        "single_feature_map = input_window_out_of_conv[:, 626, :]\n",
        "single_feature_map, single_feature_map.shape # brighter color means the embedding value is larger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2HgZ4UpdRC8"
      },
      "source": [
        "### 4.5 Permute the patch embeddings to make sure the embedding dimension is in the right order\n",
        "\n",
        "Right now, the shape of the output tensor from the conv1d layer is `(1, 768, 3)`, which is not what we want. We want the embedding dimension to be the last dimension: `(1, 3, 768)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CChg4gkebEU"
      },
      "outputs": [],
      "source": [
        "# Rearrange the dimension of the output tensor from conv1d\n",
        "patch_embeddings = input_window_out_of_conv.permute(0, 2, 1)\n",
        "patch_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVIY7c5TmqKS"
      },
      "source": [
        "### 4.6 Turning the patch embedding layer into a PyTorch module\n",
        "\n",
        "We want this module to do a few things:\n",
        "1. Create a layer to turn time series segments into embedding patches using `nn.Conv1d()`\n",
        "2. Permute the patch embeddings dimension, i.e. `(batch_size, num_patches, embedding_dim)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6SfpjeNfaKv"
      },
      "outputs": [],
      "source": [
        "# Rearrange the dimension of the output tensor from conv1d\n",
        "patch_embeddings = input_window_out_of_conv.permute(0, 2, 1)\n",
        "patch_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFyo771anKPr"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"\n",
        "  Create (multivariate) time series patches and embed the patches.\n",
        "\n",
        "  Args:\n",
        "    num_channels (int): Number of time series in the dataset.\n",
        "    patch_size (int): Length of the time series patches.\n",
        "    stride (int): Step size for sliding the kernel across the time series.\n",
        "    embedding_dim (int): Patch Embedding dimensions.\n",
        "    padding (int): Number of paddings added to the beginning and the end of the input time series windows.\n",
        "\n",
        "  Returns:\n",
        "    (torch.tensor) Time series patch embeddings\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               num_channels: int, # number of time series in the dataset (N)\n",
        "               patch_size: int, # patch length (L_p)\n",
        "               stride, # number of steps the kernel moves along the input window\n",
        "               embedding_dim: int = 768, # embedding dimension of the patch embeddings\n",
        "               padding: int = 0): # paddings added to both sides of the input window before convolutions\n",
        "    super().__init__()\n",
        "\n",
        "    # Create a layer to turn a time series input window into embedded patches\n",
        "    self.patcher = nn.Conv1d(in_channels=num_channels,\n",
        "                             out_channels=embedding_dim,\n",
        "                             kernel_size=patch_size,\n",
        "                             stride=stride,\n",
        "                             padding=padding)\n",
        "\n",
        "    self.patch_size = patch_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Check if patch size is compatible with the input window size and stride\n",
        "    input_window_size = x.shape[-1]\n",
        "    if (input_window_size + 2*self.padding - self.patch_size) % self.stride != 0:\n",
        "      print(\"Warning: Input window size is incompatible with the given patch size and stride. The last few time steps will be dropped.\") # can turn this into a warning\n",
        "\n",
        "    # Patchify the input windows and embed the patches\n",
        "    x_patched = self.patcher(x)\n",
        "\n",
        "    # Rearrange the dimension of the patches\n",
        "    return x_patched.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUC3heY1iZZC"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "The `padding` parameter in `PatchEmbedding` serves two purposes:\n",
        "1. Allows early and late steps to participate in more patches (get convolved a few more times), improving representations at boundaries.\n",
        "2. Prevents truncation by making the input window size `L_p` compatibe with the given patch size `P` and stride `S`.\n",
        "\n",
        "However, it doesn't support asymmetric padding right now (can work on it later with `F.pad`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVXYSAm3lJIS"
      },
      "outputs": [],
      "source": [
        "input_window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RdT534ykMkq"
      },
      "outputs": [],
      "source": [
        "# Test the PatchEmbedding class\n",
        "# Create an instance of a non-overlapping patch embedding layer\n",
        "non_overlap_patchify = PatchEmbedding(num_channels=2,\n",
        "                                      patch_size=2,\n",
        "                                      stride=2,\n",
        "                                      embedding_dim=768,\n",
        "                                      padding=0)\n",
        "\n",
        "# Pass a training example through patch embedding layer\n",
        "print(f\"Input sample size: {example_windows.shape}\")\n",
        "patch_embedded_example_windows = non_overlap_patchify(example_windows)\n",
        "print(f\"Output patch embeddings shape: {patch_embedded_example_windows.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbp_r9YhlrWz"
      },
      "outputs": [],
      "source": [
        "# Create an instance of an overlapping patch embedding layer\n",
        "overlap_patchify = PatchEmbedding(num_channels=2,\n",
        "                                  patch_size=2,\n",
        "                                  stride=1,\n",
        "                                  embedding_dim=768,\n",
        "                                  padding=0)\n",
        "\n",
        "# Pass a training example through patch embedding layer\n",
        "print(f\"Input sample size: {example_windows.shape}\")\n",
        "patch_embedded_example_windows = overlap_patchify(example_windows)\n",
        "print(f\"Output patch embeddings shape: {patch_embedded_example_windows.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5IudDUbn5uD"
      },
      "outputs": [],
      "source": [
        "# Create an example training sample to show how paddings can help prevent truncation\n",
        "example_long_windows = torch.randint(10, size=(2, 2, 26)).type(torch.float32)\n",
        "\n",
        "# Create an instance of a patch embedding layer with no padding\n",
        "no_pad_patchify = PatchEmbedding(num_channels=2,\n",
        "                                 patch_size=7,\n",
        "                                 stride=7,\n",
        "                                 embedding_dim=768,\n",
        "                                 padding=0) # no padding\n",
        "\n",
        "print(f\"Input sample size: {example_long_windows.shape}\")\n",
        "patch_embedded_example_long_windows_no_pad = no_pad_patchify(example_long_windows)\n",
        "print(f\"Output patch embeddings shape before padding: {patch_embedded_example_long_windows_no_pad.shape}\")\n",
        "\n",
        "# Create an instance of a patch embedding layer with padding\n",
        "padded_patchify = PatchEmbedding(num_channels=2,\n",
        "                                 patch_size=7,\n",
        "                                 stride=7,\n",
        "                                 embedding_dim=768,\n",
        "                                 padding=1) # add padding to the beginning and the end of the input\n",
        "patch_embedded_example_long_windows_with_pad = padded_patchify(example_long_windows)\n",
        "print(f\"Output patch embeddings shape after padding: {patch_embedded_example_long_windows_with_pad.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaOtPUs9pwRB"
      },
      "source": [
        "## Component 2: Patch Reprogramming\n",
        "\n",
        "Before we move on to create the patch reprogramming layer, it'd be handy to create a PyTorch module called `ToyWordEmbedding` to simulate LLM's pretrained word embeddings.\n",
        "\n",
        "Let's do some experiments with `nn.Embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebq8jLH6Sq4h"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "prompt = torch.randint(0, 10, size=(2, 5))\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-cDnbp1SSv1"
      },
      "outputs": [],
      "source": [
        "llm_word_embeddings = nn.Embedding(num_embeddings=10, # number of vocabs\n",
        "                                   embedding_dim=5) # hidden dimension of word embeddings\n",
        "llm_word_embeddings(prompt) # look up the word embeddings that corresponds to the indices in prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pdykhJIXBSv"
      },
      "outputs": [],
      "source": [
        "# Get all the word embeddings at once\n",
        "# llm_word_embeddings.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EVE3x-AQzI-"
      },
      "outputs": [],
      "source": [
        "# Create a pytorch module to mimic the frozen LLM's word embeddings\n",
        "class ToyWordEmbedding(nn.Module):\n",
        "  \"\"\"\n",
        "  Create toy embeddings to simulate the frozen LLM's pretrained word embeddings.\n",
        "\n",
        "  Args:\n",
        "    vocab_size (int): Number of vocabularies in the LLM's tokenizer.\n",
        "    hidden_size (int): Hidden dimension of the word embeddings.\n",
        "\n",
        "  Returns:\n",
        "    (torch.Tensor): A set of toy word embeddings that are not learnable (frozen).\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               vocab_size: int = 500, # number of vocabs in the tokenizer\n",
        "               hidden_size: int = 512,  # hidden dimension of the word embeddings\n",
        "               pretrained_weights: torch.Tensor = None): # pretrained word embeddings\n",
        "    super().__init__()\n",
        "\n",
        "    # Create word embedding layer\n",
        "    self.word_embedding_layer = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                             embedding_dim=hidden_size)\n",
        "\n",
        "    # Replace the random embeddings with pretrained word embeddings if there is any\n",
        "    if pretrained_weights is not None:\n",
        "      self.word_embedding_layer.weight.data.copy_(pretrained_weights)\n",
        "\n",
        "    self.word_embedding_layer.weight.requires_grad = False # freeze the toy pretrained word embeddings\n",
        "    self.weights = self.word_embedding_layer.weight\n",
        "\n",
        "  # Look up word embeddings with the given input ids\n",
        "  def forward(self, input_ids):\n",
        "    return self.word_embedding_layer(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bznhLCq5l20z"
      },
      "outputs": [],
      "source": [
        "# Test if it works\n",
        "# Create random prompt\n",
        "torch.manual_seed(42)\n",
        "random_prompt = torch.randint(10, size=(2, 5)) # 2 sentences, 5 tokens each\n",
        "\n",
        "# Create an instance of ToyWordEmbedding class\n",
        "test_word_embedding_layer = ToyWordEmbedding()\n",
        "\n",
        "# Look the corresponding embedding values by passing the prompt into the word embedding layer\n",
        "test_word_embeddings = test_word_embedding_layer(random_prompt)\n",
        "print(f\"Random prompt:\\n{random_prompt}\\n\")\n",
        "print(f\"The corresponding word embeddings:\\n{test_word_embeddings}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUuSyb7OnL8z"
      },
      "outputs": [],
      "source": [
        "# Test if the pretrained_weights work\n",
        "test_pretrained_word_embedding_layer = ToyWordEmbedding(pretrained_weights=test_word_embedding_layer.weights)\n",
        "print(f\"Pretrained word embeddings:\\n{test_pretrained_word_embedding_layer(random_prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DneCXf5NXdL5"
      },
      "source": [
        "Now we can move on to the patch reprogramming layer:\n",
        "\n",
        "Steps:\n",
        "1. Create toy LLM's pretrained word embeddings with `ToyWordEmbedding`\n",
        "2. Pass them through a linear layer to learn text prototypes (from vocab size $V$ to text prototype size $V'$)\n",
        "3. Pass the text prototypes through another linear layer to project them from hidden size $D$ to the patch embedding dimension $d_{m}$\n",
        "4. Cross attention between the patch embeddings (queries) and the learnt text prototypes (keys and values)\n",
        "5. Run the reprogrammed patch embeddings through a final linear layer to align with the LLM's backbone hidden dimension $D$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5vkczBEvZXh"
      },
      "outputs": [],
      "source": [
        "llm_word_embeddings.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wjkxst2M8nM"
      },
      "outputs": [],
      "source": [
        "class PatchReprogramming(nn.Module):\n",
        "  \"\"\"\n",
        "  Reprogram the time series patch embeddings with text prototypes before feeding\n",
        "  them into the frozen LLM to align the two different modalities (time series and text).\n",
        "\n",
        "  Args:\n",
        "    num_heads (int): Number of attention heads.\n",
        "    vocab_size (int): Number of vocabularies in the LLM's tokenizer.\n",
        "    text_prototype_size (int): Number of text prototypes.\n",
        "    hidden_size (int): Hidden dimension of the word embeddings used in the frozen LLM (D).\n",
        "    embedding_dim (int): Embedding dimension of the patch embeddings (d_m).\n",
        "    attn_dropout (float): Dropout layer used in the multi-head attention layer.\n",
        "\n",
        "  Returns:\n",
        "    (torch.tensor) Reprogrammed time series patch embeddings with hidden size\n",
        "    aligned with the frozen LLM (D).\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               num_heads: int, # number of attention heads\n",
        "               vocab_size: int = 500, # number of vocabs\n",
        "               text_prototype_size: int = 100, # number of text prototypes\n",
        "               hidden_size: int = 512, # hidden size of pretrained word embeddings\n",
        "               embedding_dim: int = 768, # embedding dimension of patch embeddings\n",
        "               attn_dropout: float = 0): # dropout layer used in attention\n",
        "    super().__init__()\n",
        "\n",
        "    # Create toy LLM's pretrained word embeddings\n",
        "    self.word_embedding_layer = ToyWordEmbedding(vocab_size=vocab_size,\n",
        "                                                 hidden_size=hidden_size)\n",
        "\n",
        "    # Create a linear layer to learn text prototypes from the pretrained word embeddings (V -> V')\n",
        "    self.text_prototype_linear_layer = nn.Linear(in_features=vocab_size,\n",
        "                                                 out_features=text_prototype_size)\n",
        "\n",
        "    # Create a linear layer to project the text prototypes to the same dimension as the patch embeddings (D -> d_m)\n",
        "    self.text_prototype_to_patch_embedding_layer = nn.Linear(in_features=hidden_size,\n",
        "                                                             out_features=embedding_dim)\n",
        "\n",
        "    # Create multi-head cross-attention layer\n",
        "    self.cross_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                            num_heads=num_heads,\n",
        "                                            dropout=attn_dropout,\n",
        "                                            batch_first=True) # use batch size as the first dimension (batch_size, num_patches, embedding_dim)\n",
        "\n",
        "    # Create a linear layer to align the reprogrammed patch embeddings with the LLM's hidden size (d_m -> D)\n",
        "    self.patch_embedding_to_llm_token_layer = nn.Linear(in_features=embedding_dim,\n",
        "                                                        out_features=hidden_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Get the weights of the pretrained word embeddings and add an extra batch size dimension\n",
        "    pretrained_word_embeddings = self.word_embedding_layer.weights.unsqueeze(dim=0).expand(x.shape[0], -1, -1) # shape: (batch_size, vocab_size, hidden_size)\n",
        "\n",
        "    # Learn text prototypes\n",
        "    text_prototypes = self.text_prototype_linear_layer(pretrained_word_embeddings.permute(0, 2, 1)) # permute the dimensions of pretrained word embeddings so that vocab size is last: (batch_size, hidden_size, vocab_size)\n",
        "\n",
        "    # Project them onto the same dimension that the patch embeddings are living in\n",
        "    text_prototypes = self.text_prototype_to_patch_embedding_layer(text_prototypes.permute(0, 2, 1)) # permute the dimensions again so that the hidden size is last: (batch_size, vocab_size, hidden_size)\n",
        "\n",
        "    # Cross-attention between text prototypes and patch embeddings\n",
        "    attn_output, _ = self.cross_attn(query=x,\n",
        "                                     key=text_prototypes,\n",
        "                                     value=text_prototypes,\n",
        "                                     need_weights=False)\n",
        "\n",
        "    # Project the attention output (reprogrammed patch embeddings) to the LLM's hidden size\n",
        "    return self.patch_embedding_to_llm_token_layer(attn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btqh4Yj0w1pS"
      },
      "outputs": [],
      "source": [
        "# Test if it works\n",
        "print(f\"Patch embeddings shape: {patch_embeddings.shape}\\n\")\n",
        "# Create an instance of PatchReprogamming layer\n",
        "patch_reprogramming_layer = PatchReprogramming(num_heads=12)\n",
        "\n",
        "# Reprogram the patch embeddings\n",
        "reprogrammed_patch_embeddings = patch_reprogramming_layer(patch_embeddings)\n",
        "print(f\"Reprogrammed patch embeddings:\\n{reprogrammed_patch_embeddings}\")\n",
        "print(f\"Shape (batch_size, num_patches, embedding_dim) -> (batch_size, num_patches, hidden_size): {reprogrammed_patch_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NVUvevvxmA5"
      },
      "source": [
        "## Component 3: Prompt-as-Prefix\n",
        "\n",
        "We'll skip this part for now because it's not necessary at this point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkyPYAE91ZSI"
      },
      "source": [
        "## Component 4: Output Projection\n",
        "\n",
        "With the reprogrammed patch embeddings ready, it's time to pass them (prepended with prompt) into the frozen LLM. For simplicity, instead of loading a pretrained model from Hugging Face, we'll build a toy PyTorch module that mimics a decoder-only LLM.\n",
        "\n",
        "We can build the module with the following steps:\n",
        "1. Create a multi-head attention block (multi-head attenton and add&layer norm)\n",
        "2. Create a mlp block (mlp and add&layer norm)\n",
        "3. Put everything together to create a transformer decoder block\n",
        "4. Stack them together to mimic the decoder-only LLM structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oScL0SjMRGGf"
      },
      "outputs": [],
      "source": [
        "all_ones = torch.ones((5, 5), dtype=bool, device=DEVICE)\n",
        "all_ones, all_ones.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0E1BER_RV-a"
      },
      "outputs": [],
      "source": [
        "torch.triu(all_ones, diagonal=1).device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgQ9WnEemZ9F"
      },
      "source": [
        "### Create a Multi-head attention Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezuTcMLOKCI3"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Create a multi-head casual self-attention block (\"causal MSA block\" for short).\n",
        "\n",
        "  Returns:\n",
        "    (torch.Tensor): Contextualised word embeddings (from Prompt-as-Prefix) and time series patch embeddings\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               hidden_size: int = 512, # Hidden size (D)\n",
        "               num_heads: int = 16, # Number of attention heads\n",
        "               attn_dropout: float = 0): # Dropout layer used in attention\n",
        "    super().__init__()\n",
        "\n",
        "    # Create the norm layer (LN)\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=hidden_size)\n",
        "\n",
        "    # Create multi-head self-attention (MSA) layer\n",
        "    self.multihead_attn = nn.MultiheadAttention(embed_dim=hidden_size,\n",
        "                                                num_heads=num_heads,\n",
        "                                                dropout=attn_dropout,\n",
        "                                                batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Get the number of tokens fed to the LLM\n",
        "    num_tokens = x.shape[1]\n",
        "\n",
        "    # Create a casual mask (upper triangular matrix) of shape (num_tokens, num_tokens)\n",
        "    causal_mask = torch.triu(\n",
        "        torch.ones((num_tokens, num_tokens), device=x.device, dtype=bool), # boolean matrix\n",
        "        diagonal=1 # upper triangular excluding diagonal\n",
        "    )\n",
        "\n",
        "    # Run the input embeddings (word + time series patch embeddings) through a masked attention layer\n",
        "    attn_output, _ = self.multihead_attn(query=x,\n",
        "                                         key=x,\n",
        "                                         value=x,\n",
        "                                         attn_mask=causal_mask,\n",
        "                                         need_weights=False,\n",
        "                                         is_causal=True) # causal attention\n",
        "\n",
        "    # Normalise the attention output\n",
        "    return self.layer_norm(attn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxSLRX6tT2Fw"
      },
      "outputs": [],
      "source": [
        "# Create an instance of MSA block\n",
        "multihead_masked_attention_block = MultiHeadAttentionBlock()\n",
        "\n",
        "# Pass the repogrammed patch embeddings through MSA block\n",
        "reprogrammed_patch_embeddings_through_msa_block = multihead_masked_attention_block(reprogrammed_patch_embeddings)\n",
        "print(f\"Input shape of MSA block: {reprogrammed_patch_embeddings.shape}\")\n",
        "print(f\"Output shape of MSA block: {reprogrammed_patch_embeddings_through_msa_block.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG7em8MyhiHs"
      },
      "outputs": [],
      "source": [
        "reprogrammed_patch_embeddings.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AVrc4PthCsv"
      },
      "outputs": [],
      "source": [
        "next(iter(multihead_masked_attention_block.parameters())).device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72SLDwoNU7SP"
      },
      "outputs": [],
      "source": [
        "reprogrammed_patch_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek3LvX6uV_7i"
      },
      "outputs": [],
      "source": [
        "reprogrammed_patch_embeddings_through_msa_block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFNBQw4pWDy-"
      },
      "source": [
        "### Creata a MLP block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O005dEKpWStr"
      },
      "outputs": [],
      "source": [
        "class MLPBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               hidden_size: int = 512,\n",
        "               mlp_size: int = 3072,\n",
        "               dropout: float = 0):\n",
        "    \"\"\"\n",
        "    Create a fully connected layer block (\"MLP block\" for short).\n",
        "\n",
        "    Notes:\n",
        "      The MLP architecture (and the default parameters) used here is based on the one used in the Vision Transformer (ViT) paper.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Create the norm layer (LN)\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=hidden_size)\n",
        "\n",
        "    # Create the MLP\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=hidden_size,\n",
        "                  out_features=mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features=mlp_size,\n",
        "                  out_features=hidden_size),\n",
        "        nn.Dropout(p=dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_norm(self.mlp(x)) # operation fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIv14lTVYLHl"
      },
      "outputs": [],
      "source": [
        "# Create an instance of MLPBlock\n",
        "mlp_block = MLPBlock()\n",
        "\n",
        "# Pass the output from MSABlock through MLPBlock\n",
        "reprogrammed_patch_embeddings_through_mlp_block = mlp_block(reprogrammed_patch_embeddings_through_msa_block)\n",
        "print(f\"Input shape of MLP block: {reprogrammed_patch_embeddings_through_msa_block.shape}\")\n",
        "print(f\"Output shape of MLP block: {reprogrammed_patch_embeddings_through_mlp_block.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FnCN1uJY3Wu"
      },
      "outputs": [],
      "source": [
        "reprogrammed_patch_embeddings_through_msa_block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7rlcH_HY6u3"
      },
      "outputs": [],
      "source": [
        "reprogrammed_patch_embeddings_through_mlp_block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUYo_19iY8Qa"
      },
      "source": [
        "The transformer decoder in the LLM is a combination of alternating blocks of MSA and MLP.\n",
        "\n",
        "Let's create a custom Transformer Decoder block:\n",
        "\n",
        "> Why don't we use the in-built PyTorch transformer decoder layer (https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html)?\n",
        "Because it'll help experimenting and debugging when we develop ViTimeLLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P7ri2gzZPz3"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  A transformer decoder class which is made of alternating MSA block and MLP block.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               hidden_size: int = 512,\n",
        "               num_heads: int = 16,\n",
        "               mlp_size: int = 3072,\n",
        "               mlp_dropout: float = 0.0,\n",
        "               attn_dropout: float = 0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    # Create MSA block\n",
        "    self.msa_block = MultiHeadAttentionBlock(hidden_size=hidden_size,\n",
        "                                             num_heads=num_heads,\n",
        "                                             attn_dropout=attn_dropout)\n",
        "\n",
        "    # Create MLP block\n",
        "    self.mlp_block = MLPBlock(hidden_size=hidden_size,\n",
        "                              mlp_size=mlp_size,\n",
        "                              dropout=mlp_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.msa_block(x) + x # residual connection\n",
        "    x = self.mlp_block(x) + x\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc1LYzKYiQrh"
      },
      "outputs": [],
      "source": [
        "# Create an instance of TransformerDecoderBlock\n",
        "transformer_decoder_block = TransformerDecoderBlock()\n",
        "next(iter(transformer_decoder_block.parameters())).device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O1FPvXfjYLP"
      },
      "outputs": [],
      "source": [
        "reprogrammed_patch_embeddings.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VraISneeLba"
      },
      "outputs": [],
      "source": [
        "# Create an instance of TransformerDecoderBlock\n",
        "transformer_decoder_block = TransformerDecoderBlock()\n",
        "\n",
        "# Get a summary using torchinfo\n",
        "# summary(model=transformer_decoder_block, # note: torchinfo.summary secretly moves your model to cuda!\n",
        "#         input_size=(1, 3, 512),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])\n",
        "\n",
        "x_output = transformer_decoder_block(reprogrammed_patch_embeddings)\n",
        "x_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUqbusqh3oYK"
      },
      "outputs": [],
      "source": [
        "flatten_layer = nn.Flatten(start_dim=1)\n",
        "x_output_flattened = flatten_layer(x_output)\n",
        "x_output_flattened.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLcnYFft3xbF"
      },
      "outputs": [],
      "source": [
        "3*512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ3KeubxlaRr"
      },
      "source": [
        "### Putting it all together to create Toy LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8CI4icHmikR"
      },
      "outputs": [],
      "source": [
        "class ToyLLM(nn.Module):\n",
        "  \"\"\"\n",
        "  A toy LLM consisting of multiple stacked transformer decoder blocks.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               num_transformer_layers: int = 12,\n",
        "               hidden_size: int = 512,\n",
        "               mlp_size: int = 3072,\n",
        "               num_heads: int = 16,\n",
        "               attn_dropout: float = 0.0,\n",
        "               mlp_dropout: float = 0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    # Create the Transformer Decoder blocks\n",
        "    self.transformer_decoders = nn.Sequential(*[TransformerDecoderBlock(hidden_size,\n",
        "                                                                       num_heads=num_heads,\n",
        "                                                                       mlp_size=mlp_size,\n",
        "                                                                       mlp_dropout=mlp_dropout,\n",
        "                                                                       attn_dropout=attn_dropout) for _ in range(num_transformer_layers)])\n",
        "  def forward(self, x):\n",
        "    return self.transformer_decoders(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHGvO8xKotqc"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create an instance of ToyLLM\n",
        "toyllm = ToyLLM()\n",
        "\n",
        "# Freeze every parameter in the toy LLM\n",
        "for p in toyllm.parameters():\n",
        "  p.requires_grad = False\n",
        "\n",
        "# Pass an example reprogrammed patch embeddings to our ToyLLM\n",
        "output_embeddings = toyllm(reprogrammed_patch_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zifT5D60Gq0v"
      },
      "outputs": [],
      "source": [
        "reprogrammed_patch_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bvffFNoF7xd"
      },
      "outputs": [],
      "source": [
        "# Check if the parameters in the toy LLM are frozen using torchinfo.summary\n",
        "# summary(model=toyllm,\n",
        "#         input_size=(1, 3, 512), # (batch_size, num_patches, embedding_dim)\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT7ZgVgCpAvX"
      },
      "source": [
        "### Output projection\n",
        "\n",
        "Recall:\n",
        "> Output Projection. Upon packing and feedforwarding the prompt and patch embeddings $\\mathbf{O}^{(i)}$ through the frozen LLM as shown in Fig. 2, we discard the prefixal part and obtain the output representations. Following this, we flatten and linear project them to derive the final forecasts $\\hat{\\mathbf{Y}}^{(i)}$.\n",
        "\n",
        "With the toy LLM ready, it's eventually time to put all the puzzles together. Inside our `TimeLLM` model class, we want:\n",
        "1. `RevIN`: Normalise the input time windows/ re-scale the output forecast values\n",
        "2. `PatchEmbedding` - Patchify the input and embed them to patch embeddings with dimension $d_m$\n",
        "3. `PatchReprogramming` - Learn text prototypes and then reprogram the time series patch embeddings to align with the source representation space (text)\n",
        "4. `ToyLLM` - Pass the reprogrammed patch embeddings to the frozen LLM\n",
        "5. Discard the prefixal part (prompt) and flatten the output embeddings\n",
        "6. Project them to final forecast with a linear head\n",
        "7. Rescale the forecast with `RevIN.inverse()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTAvC8-fphVO"
      },
      "outputs": [],
      "source": [
        "class TimeLLM(nn.Module):\n",
        "  \"\"\"\n",
        "  The TimeLLM architecture replicated from the TimeLLM paper.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               num_channels: int,\n",
        "               window_size: int,\n",
        "               forecast_horizon: int,\n",
        "               patch_size: int,\n",
        "               stride: int,\n",
        "               affine: bool = False,\n",
        "               eps: float = 1e-5,\n",
        "               embedding_dim: int = 768,\n",
        "               padding: int = 0,\n",
        "               reprogramming_head: int = 12,\n",
        "               vocab_size: int = 500,\n",
        "               text_prototype_size: int = 100,\n",
        "               hidden_size: int = 512,\n",
        "               reprogramming_attn_dropout: float = 0.0,\n",
        "               num_transformer_layers: int = 12,\n",
        "               llm_msa_heads: int = 16,\n",
        "               llm_mlp_size: int = 3072,\n",
        "               llm_mlp_dropout: float = 0.0,\n",
        "               llm_attn_dropout: float = 0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_channels = num_channels\n",
        "    self.forecast_horizon = forecast_horizon\n",
        "\n",
        "    # Calculate the number of patches\n",
        "    self.num_patches = (window_size + 2*padding - patch_size) // stride + 1\n",
        "\n",
        "    # Create a RevIN layer\n",
        "    self.revin = RevIN(num_channels,\n",
        "                       affine=False)\n",
        "\n",
        "    # Create patch embedding layer\n",
        "    self.patch_embedding_layer = PatchEmbedding(num_channels=num_channels,\n",
        "                                                patch_size=patch_size,\n",
        "                                                stride=stride,\n",
        "                                                embedding_dim=embedding_dim,\n",
        "                                                padding=padding)\n",
        "\n",
        "    # Create Patch reprogramming layer\n",
        "    self.patch_reprogramming_layer = PatchReprogramming(num_heads=reprogramming_head,\n",
        "                                                        vocab_size=vocab_size,\n",
        "                                                        text_prototype_size=text_prototype_size,\n",
        "                                                        hidden_size=hidden_size,\n",
        "                                                        embedding_dim=embedding_dim,\n",
        "                                                        attn_dropout=reprogramming_attn_dropout)\n",
        "    # Create toy LLM\n",
        "    self.toy_llm = ToyLLM(num_transformer_layers=num_transformer_layers,\n",
        "                          hidden_size=hidden_size,\n",
        "                          mlp_size=llm_mlp_size,\n",
        "                          num_heads=llm_msa_heads,\n",
        "                          attn_dropout=llm_attn_dropout,\n",
        "                          mlp_dropout=llm_mlp_dropout)\n",
        "\n",
        "    # Freeze all the parameters in the toy LLM\n",
        "    for p in self.toy_llm.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "    # Create a flatten layer\n",
        "    self.flatten_layer = nn.Flatten(start_dim=1)\n",
        "\n",
        "    # Create a forecast head\n",
        "    self.forecast_head = nn.Linear(in_features=self.num_patches*hidden_size,\n",
        "                                   out_features=num_channels*forecast_horizon)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 1. Normalise the input time window\n",
        "    x_norm = self.revin(x)\n",
        "\n",
        "    # 2. Turn them into patch embeddings\n",
        "    patch_embeddings = self.patch_embedding_layer(x_norm)\n",
        "\n",
        "    # 3. Reprogram the patch embeddings\n",
        "    reprogrammed_patch_embeddings = self.patch_reprogramming_layer(patch_embeddings)\n",
        "\n",
        "    # 4. Pass it to the frozen LLM\n",
        "    output_embeddings = self.toy_llm(reprogrammed_patch_embeddings)\n",
        "\n",
        "    # 5. Flatten the output embeddings\n",
        "    flattened_output_embeddings = self.flatten_layer(output_embeddings)\n",
        "\n",
        "    # 6. Project them to get normalised forecast values\n",
        "    y_norm = self.forecast_head(flattened_output_embeddings).view(x.shape[0], self.num_channels, self.forecast_horizon)\n",
        "\n",
        "    # 7. Return the rescaled forecast values\n",
        "    return self.revin.inverse(y_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7Ab0RJZe2a6"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(size=(1, 4))\n",
        "print(x)\n",
        "print(x.view(1, 2, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFAXcuzny-tj"
      },
      "outputs": [],
      "source": [
        "# Test the TimeLLM class\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a random input window tensor\n",
        "random_input_window = torch.randn(1, 2, 6)\n",
        "\n",
        "# Create an instance of TimeLLM\n",
        "timellm = TimeLLM(num_channels=2,\n",
        "                  window_size=6,\n",
        "                  forecast_horizon=1,\n",
        "                  patch_size=2,\n",
        "                  stride=2)\n",
        "\n",
        "# Pass the input window tensor to TimeLLM\n",
        "forecast = timellm(random_input_window)\n",
        "print(f\"[INFO] Model output: {forecast} | Shape: {forecast.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm6ubEeg9Se0"
      },
      "outputs": [],
      "source": [
        "# Get a summary of our TimeLLM model\n",
        "# summary(model=timellm,\n",
        "#         input_size=(1, 2, 6),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS6DKpL69lvJ"
      },
      "source": [
        "## 5. Setting up training code for our custom TimeLLM\n",
        "\n",
        "We've replicated the TimeLLM architecture (with a toy LLM), now let's set up training codes to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FjBHFlMPwg9"
      },
      "source": [
        "### 5.1 Creating an optimizer\n",
        "\n",
        "The paper states it uses the Adam optimizer (section B.4 Model Configurations), but it doesn't mention the values for $B1$ and $B2$, so we'll just stick to the default values for now.\n",
        "\n",
        "The paper uses different learning rates for different datasets. For now, we'll just use `10^-3`, which is used in the LTF-ETTh1 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRNXpW1EP55K"
      },
      "source": [
        "### 5.2 Creating a loss function\n",
        "\n",
        "The paper mentions it uses mean square error (MSE) as its loss function, we can implement it with `torch.nn.MSELoss()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU-ZEyurUmcI"
      },
      "outputs": [],
      "source": [
        "# timellm.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAvskd1vS47b"
      },
      "outputs": [],
      "source": [
        "# Setup loss function and optimizer\n",
        "optimizer = torch.optim.Adam(params=timellm.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVp25bBZU1pU"
      },
      "source": [
        "### 5.3 Setup training code\n",
        "\n",
        "Let's create training and testing codes inside a function so that they can be reused in the future.\n",
        "\n",
        "TODO:\n",
        "- Do a three split instead (training-validation-test)\n",
        "- Compile the model\n",
        "- Use AMP\n",
        "- Use `tf32` instead of `float32` (PyTorch's default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0Xfuty9YruT"
      },
      "outputs": [],
      "source": [
        "# Get an example from the training dataloader\n",
        "example = next(iter(train_dataloader_custom))\n",
        "\n",
        "# Print out some info of the example\n",
        "print(f\"Training sample:\\n{example[0]}\")\n",
        "print(f\"Shape of training sample:\\n{example[0].shape}\")\n",
        "print()\n",
        "print(f\"Label:{example[1]} | Shape: {example[1].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWYWo--RYhPS"
      },
      "outputs": [],
      "source": [
        "forecast, forecast.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKSfFSx7lt46"
      },
      "outputs": [],
      "source": [
        "label = torch.rand(size=(1, 2, 1))\n",
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWyxgwNnmjWy"
      },
      "outputs": [],
      "source": [
        "loss_fn(label, forecast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xctWgzFuVHmd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Create training step function\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> float:\n",
        "  \"\"\"\n",
        "  Trains a PyTorch (regression) model for a single epoch.\n",
        "\n",
        "  Args:\n",
        "    model (torch.nn.Module): A PyTorch (regression) model to be trained.\n",
        "    dataloader (torch.utils.data.DataLoader): A DataLoader instance for the model to be trained on.\n",
        "    loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
        "    optimizer (torch.optim.Optimizer): A PyTorch optimizer to minimize the loss function.\n",
        "    device (torch.device): A target device to compute on (e.g. \"cuda\", \"mps\" or \"cpu\")\n",
        "  Returns:\n",
        "    (float) Average training loss across the epoch.\n",
        "  \"\"\"\n",
        "  # Put the model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss\n",
        "  train_loss = 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Send data to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Calculate and accumulate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "  # Adjust metrics to get average loss per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  return train_loss\n",
        "\n",
        "# Create test step function\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> float:\n",
        "  \"\"\"\n",
        "  Tests a PyTorch (regression) model for a single epoch.\n",
        "\n",
        "  Args:\n",
        "    model (torch.nn.Module): A PyTorch (regression) model to be tested.\n",
        "    dataloader (torch.utils.data.DataLoader): A DataLoader instance for the model to be trained on.\n",
        "    loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
        "    device (torch.device): A target device to compute on (e.g. \"cuda\", \"mps\" or \"cpu\")\n",
        "\n",
        "  Returns:\n",
        "    (float) Average test loss across the epoch.\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss\n",
        "  test_loss = 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "    # Loop through DataLoader batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Calculate and accumulate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "  # Adjust metrics to get average loss\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  return test_loss\n",
        "\n",
        "# Create a training function with train step and test step combined\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "  \"\"\"\n",
        "  Trains and tests a PyTorch model.\n",
        "\n",
        "  Returns:\n",
        "    (Dict) A dictionary of training and testing loss. Each metric has a value in a list for\n",
        "      each epoch.\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\n",
        "      \"train_loss\": [],\n",
        "      \"test_loss\": []\n",
        "  }\n",
        "\n",
        "  # Make sure model on target device\n",
        "  model.to(device)\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss = train_step(model=model,\n",
        "                            dataloader=train_dataloader,\n",
        "                            loss_fn=loss_fn,\n",
        "                            optimizer=optimizer,\n",
        "                            device=device)\n",
        "    test_loss = test_step(model=model,\n",
        "                          dataloader=test_dataloader,\n",
        "                          loss_fn=loss_fn,\n",
        "                          device=device)\n",
        "\n",
        "    # Print out what's happening\n",
        "    print(\n",
        "        f\"Epoch: {epoch+1} | \"\n",
        "        f\"train_loss: {train_loss:.4f} | \"\n",
        "        f\"test_loss: {test_loss:.4f} | \"\n",
        "    )\n",
        "\n",
        "    # Update results dictionary\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBtvCi3oPt2D"
      },
      "outputs": [],
      "source": [
        "# Train our timellm model\n",
        "result = train(model=timellm,\n",
        "               train_dataloader=train_dataloader_custom,\n",
        "               test_dataloader=test_dataloader_custom,\n",
        "               optimizer=optimizer,\n",
        "               loss_fn=loss_fn,\n",
        "               epochs=6,\n",
        "               device=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHkVesf6RkEU"
      },
      "source": [
        "## 6. Get predictions on test/custom data\n",
        "\n",
        "Now we've trained our TimeLLM model! It's time to predict on the test set/custom data.\n",
        "\n",
        "Let's create a function called `pred_and_plot` to make predictions and plot them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s1w-bULoTdMw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "def pred_and_plot(model: torch.nn.Module,\n",
        "                  dataset: pd.DataFrame,\n",
        "                  columns: Optional[List[str]] = None,\n",
        "                  batch_size: int = 3,\n",
        "                  window_size: int = 6,\n",
        "                  forecast_horizon: int = 1,\n",
        "                  stride: int = 5,\n",
        "                  device: torch.device = DEVICE) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  Run a trained PyTorch (regression) model of time series and plot predictions vs. ground truth.\n",
        "\n",
        "  Args:\n",
        "    model (torch.nn.Module): A trained PyTorch time series forecasting model.\n",
        "    dataset (pd.DataFrame): The target DataFrame you want to run inference on.\n",
        "    columns (Optional[List[str]]): Target columns included in the plot.\n",
        "    batch_size (int): Number of samples in each batch.\n",
        "    window_size (int): Number of time steps included in each time series window.\n",
        "    forecast_horizon (int): Number of time steps into the future you want the model to predict.\n",
        "    stride (int): Step size for the sliding window over the time series.\n",
        "    device (torch.device): A target device to run inference on (e.g. \"cuda\", \"mps\" or \"cpu\")\n",
        "\n",
        "  Returns:\n",
        "    (Tuple[torch.Tensor, torch.Tensor]) A tuple of ground truth labels and model predictions\n",
        "      in the form of tensor. They are of shape (`num_channels`, `len_pred`), where `num_channels` is\n",
        "      the number of time series in the dataset and `len_pred` is the length of model predictions.\n",
        "  \"\"\"\n",
        "  # Create empty lists to store labels and predictions\n",
        "  labels = []\n",
        "  predictions = []\n",
        "\n",
        "  # Calculate number of workers\n",
        "  num_workers = os.cpu_count()\n",
        "\n",
        "  # Turn the raw Dataframe into an instance of TimeSeriesDatasetCustom\n",
        "  custom_dataset = TimeSeriesDatasetCustom(df=dataset,\n",
        "                                           window_size=window_size,\n",
        "                                           forecast_horizon=forecast_horizon,\n",
        "                                           stride=stride)\n",
        "\n",
        "  # Turn the custom pytorch Datasets into DataLoaders\n",
        "  dataloader = DataLoader(dataset=custom_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          num_workers=num_workers,\n",
        "                          shuffle=False,\n",
        "                          drop_last=False) # don't drop the last batch\n",
        "\n",
        "  # Make sure the model is on the target device\n",
        "  model.to(device)\n",
        "\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Turn on inference manager\n",
        "  with torch.inference_mode():\n",
        "    # Loop through DataLoader batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Append the label to a list\n",
        "      labels.append(y)\n",
        "\n",
        "      # Append the prediction to a list\n",
        "      predictions.append(y_pred)\n",
        "\n",
        "  # Concatenate predictions along the batch dimension\n",
        "  all_labels = torch.cat(labels, dim=0)\n",
        "  all_preds = torch.cat(predictions, dim=0)\n",
        "\n",
        "  # Permute the batch size and number of channels and then flatten them -> (num_channels, batch_size*forecast_horizon)\n",
        "  all_labels = all_labels.permute(1, 0, 2).flatten(start_dim=1)\n",
        "  all_preds = all_preds.permute(1, 0, 2).flatten(start_dim=1)\n",
        "\n",
        "  # Plot the graph\n",
        "  # Check if any columns are selected, if not, defaults to all columns\n",
        "  if columns is None:\n",
        "    columns = list(dataset.columns)\n",
        "\n",
        "  # Get the index of the selected columns in the dataframe column list\n",
        "  column_index_list = [list(dataset.columns).index(column) for column in columns]\n",
        "\n",
        "  # Create column names list for the labels and predictions dataframe\n",
        "  column_names = columns + [f\"{column}_pred\" for column in columns]\n",
        "\n",
        "  # Select the specificed columns' labels and predictions\n",
        "  selected_labels = all_labels[column_index_list, :]\n",
        "  selected_preds = all_preds[column_index_list, :]\n",
        "\n",
        "  # Concatenate the labels and predictions along the columns\n",
        "  label_and_pred_tensor = torch.cat([selected_labels.transpose(0, 1), selected_preds.transpose(0, 1)], dim=1)\n",
        "\n",
        "  # Create a DataFrame to store the labels and predictions\n",
        "  label_and_pred_df = pd.DataFrame(data=label_and_pred_tensor.cpu(),\n",
        "                                   columns=column_names) # need to move the tensor to cpu because matplotlib works with numpy arrays\n",
        "\n",
        "  # Plot the labels and predictions\n",
        "  label_and_pred_df.plot();\n",
        "\n",
        "  return all_labels, all_preds # each of them has shape: (num_channels, len_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPHx29l28Ske"
      },
      "outputs": [],
      "source": [
        "labels_and_preds = pred_and_plot(model=timellm,\n",
        "                                 dataset=train_df)\n",
        "labels, preds = labels_and_preds"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyP5rig0xb8+bsndp5rfatIT",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
