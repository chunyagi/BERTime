{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47RDbUeRIbZA"
   },
   "source": [
    "# Time-LLM Paper Replicating\n",
    "\n",
    "In this notebook, we're going to replicate the full TimeLLM architecture with a real LLM, DistilGPT2 (for fast experiments) with PyTorch: https://arxiv.org/abs/2310.01728."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0QTINU6dR9h"
   },
   "source": [
    "## 0. Get setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7539,
     "status": "ok",
     "timestamp": 1751128434058,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "htdoV6kCgFWj"
   },
   "outputs": [],
   "source": [
    "# Import libraries that are pre-installed in google colab\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import transformers, accelerate\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from typing import List, Tuple, Dict, Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6249,
     "status": "ok",
     "timestamp": 1751128440365,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "zASBXiLahNAZ",
    "outputId": "0a6ca7b2-f7dd-4269-d99a-5fd35eb98665"
   },
   "outputs": [],
   "source": [
    "# Install torchinfo\n",
    "try:\n",
    "  from torchinfo import summary\n",
    "except:\n",
    "  print(\"[INFO] Couldn't find torchinfo, installing it...\")\n",
    "  !pip3 install -q torchinfo\n",
    "  from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1751128440393,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "0-xv99TEh8Yf",
    "outputId": "98574c6c-a2d3-4404-f200-17f5c351fa61"
   },
   "outputs": [],
   "source": [
    "# Setup device agnostic code\n",
    "def set_device():\n",
    "  if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "  elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = \"mps\"\n",
    "  else:\n",
    "    device = \"cpu\"\n",
    "  return device\n",
    "\n",
    "DEVICE = set_device()\n",
    "print(f\"[INFO] Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIJjTQSKjRaz"
   },
   "source": [
    "## 1. Get data\n",
    "\n",
    "For simplicity, we're going to create a toy dataset.\n",
    "\n",
    "Now that our TimeLLM implementation is verified (see the previous notebook: https://github.com/chunyagi/ViTimeLLM/blob/main/timellm_paper_replicating.ipynb), let's test out the **prompt-as-prefix** mechanism by:\n",
    "\n",
    "1. Extending the toy dataset to more than one cycle\n",
    "2. Adding Gaussian noise to each time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1751128440531,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "nYl4Ks_zlJxs",
    "outputId": "27d9e1dc-9d2f-4755-c918-f64a4b38b202"
   },
   "outputs": [],
   "source": [
    "# For completeness\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setups of the toy dataset\n",
    "PERIODS = 24 * 7 # 1 week per cycle\n",
    "FREQ = \"h\" # sampling frequency\n",
    "N_CYCLES = 5 # number of full cycles in the dataset\n",
    "SIGMA = 0.2 # standard deviance of the gaussian noise\n",
    "\n",
    "total_period = PERIODS * N_CYCLES # calculate the total number of periods in the dataset\n",
    "\n",
    "# Create datatime index\n",
    "dates = pd.date_range(\"2025-01-01\", periods=total_period, freq=FREQ)\n",
    "\n",
    "# Create sine and cosine curves (underlying DGP)\n",
    "sin_curve = np.sin(np.linspace(0, 2 * np.pi * N_CYCLES, total_period))\n",
    "cos_curve = np.cos(np.linspace(0, 2 * np.pi * N_CYCLES, total_period))\n",
    "\n",
    "# Create gaussian noise\n",
    "noise = np.random.normal(loc=0.0,\n",
    "                         scale=SIGMA,\n",
    "                         size=sin_curve.shape)\n",
    "\n",
    "# Add gaussian noise to the data\n",
    "sin_data = sin_curve + noise\n",
    "cos_data = cos_curve + noise\n",
    "\n",
    "# Create toy dataset\n",
    "toy_df = pd.DataFrame(\n",
    "    {\"sin_data\": sin_data,\n",
    "     \"cos_data\": cos_data},\n",
    "    index=dates\n",
    ")\n",
    "toy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1751128440560,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "_8bGgqPdJlAk",
    "outputId": "dc0599eb-36ba-472d-a285-8ff4bc991ca7"
   },
   "outputs": [],
   "source": [
    "len(toy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1751128441142,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "VyCfZ3sfuujZ",
    "outputId": "9f75d8df-5341-490d-ed55-dfee4b7e5cd7"
   },
   "outputs": [],
   "source": [
    "# Visualise data\n",
    "toy_df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZ0V_qSNuv6I"
   },
   "source": [
    "## 2. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSYpHrlAMzIZ"
   },
   "source": [
    "### 2.1 Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1751128441150,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "Am81c6eRv9jr",
    "outputId": "c4d68348-59ca-4ac3-f90f-66098407e07b"
   },
   "outputs": [],
   "source": [
    "# Let's split the data into training and test sets\n",
    "train_split = int(len(toy_df) * 0.8) # 4 cycles for training, 1 cycle for testing\n",
    "train_df = toy_df[:train_split]\n",
    "test_df = toy_df[train_split:]\n",
    "print(f\"[INFO] Size of the training data: {len(train_df)}\")\n",
    "print(f\"[INFO] Size of the test data: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1751128441205,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "qyADrtXXM_xH",
    "outputId": "26907230-f3b2-4073-e8e8-3cbcfddb0a45"
   },
   "outputs": [],
   "source": [
    "672 / 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlah_vwWKMpd"
   },
   "source": [
    "### 2.2 Create a custom PyTorch `Dataset` to replicate `TimeSeriesDataset` from PyTorch Forecasting\n",
    "\n",
    "To create our own custom dataset, we want to:\n",
    "1. Subclass `torch.utils.data.Dataset`.\n",
    "2. Init our subclass with our target dataframe, (sliding) window size, forecast horizon, stride as well as a transform if we'd like to apply global transform (e.g. standardization) on our data (not necessary here).\n",
    "3. Create several attributes:\n",
    "* `df` - target dataset\n",
    "* `window_size` - size of the sliding window, also called the history length (number of data points in the past the model uses to predict the future values)\n",
    "* `forecast_horizon` - number of steps ahead the model wants to predict in the future\n",
    "* `stride` - step size for sliding the input window across the time series\n",
    "* `num_samples` - number of training samples\n",
    "* `transform` - the (global) transform we'd like to use (not necessary here, just for completeness)\n",
    "4. Overwrite the `__len__()` method to return the length of our dataset\n",
    "5. Overwrite the `__getitem__()` method to return a given sample pair (in the form of a `Tuple`, `(input_window, target)`) when passed an index.\n",
    "\n",
    "**Note:**\n",
    "- Window size $\\neq$ patch size! One input window could correspond to more than one patch!\n",
    "- The `stride` here is not the same as the stride used in the time series patchification later!\n",
    "- Given an index, want the `__getitem__()` method to return a tuple `(input_window, target)`\n",
    " - `input_window` is a tensor of shape `(number_of_channels, window_size)`\n",
    " - `target` is a tensor of shape `(number_of_channels, forcast_horizon)`, where `num_of_channels` means how many time series there are in the dataset.\n",
    "\n",
    "> Remark: Some data points at the end of the series will be dropped when `(len(df) - window_size - forecast_horizon) % stride != 0`, because we don't have enough data points to form a complete (training/test) sample with the remaining data points. To make this problem minimal, try using overlapping sliding windows (i.e. `stride` < `window_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1751128441210,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "Z695D4IPNeHW"
   },
   "outputs": [],
   "source": [
    "# Write a custom dataset class and do the import\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple, Dict, Callable, Optional\n",
    "\n",
    "# Subclass torch.utils.data.Dataset\n",
    "class TimeSeriesDatasetCustom(Dataset):\n",
    "  # Initialize our custom dataset\n",
    "  def __init__(self,\n",
    "               df: pd.DataFrame,\n",
    "               window_size: int,\n",
    "               forecast_horizon: int = 1,\n",
    "               stride: int = 1,\n",
    "               transform: Optional[Callable[[torch.tensor], torch.tensor]] = None): # not necessary, just for completeness\n",
    "    super().__init__() # optinal here, but doesn't hurt\n",
    "\n",
    "    # Create class attributes\n",
    "    self.df = df\n",
    "    self.window_size = window_size\n",
    "    self.forecast_horizon = forecast_horizon\n",
    "    self.stride = stride\n",
    "    self.transform = transform\n",
    "\n",
    "  # Overwrite __len__()\n",
    "  def __len__(self) -> int:\n",
    "    \"Returns the total number of training samples.\"\n",
    "    # Calcuate number of training samples\n",
    "    num_samples = (len(self.df) - self.window_size - self.forecast_horizon) // self.stride + 1\n",
    "    return num_samples\n",
    "\n",
    "  # Overwrite __getitem__() method to return a particular sample pair (input_window, target)\n",
    "  def __getitem__(self, index: int) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"Returns one sample of data, data and label (x, y).\"\n",
    "    # Normalise negative indicies\n",
    "    if index < 0:\n",
    "      index = len(self) + index\n",
    "\n",
    "    start = index * self.stride\n",
    "    x = self.df[start: start + self.window_size].to_numpy()\n",
    "    y = self.df[start + self.window_size: start + self.window_size + self.forecast_horizon].to_numpy()\n",
    "\n",
    "    # Convert X & y to tensors\n",
    "    x = torch.from_numpy(x).type(torch.float32) # convert the datatype from float64 (Numpy's default dtype) to float32 (PyTorch's default dtype)\n",
    "    y = torch.from_numpy(y).type(torch.float32)\n",
    "\n",
    "    # Change the output shape of x (pytorch prefers channel-first)\n",
    "    if x.ndim == 1: # univariate time series data\n",
    "      x = x.unsqueeze(dim=0) # add channel size\n",
    "    else: # multivariate time series data\n",
    "      x = x.transpose(0, 1) # channel first\n",
    "\n",
    "    # Change the output shape of y as well\n",
    "    if y.ndim == 1:\n",
    "      y = y.unsqueeze(dim=0) # add channel size\n",
    "    else:\n",
    "      y = y.transpose(0, 1) # channel first\n",
    "\n",
    "    # Transform if necessary\n",
    "    if self.transform:\n",
    "      return self.transform(x), y\n",
    "    else:\n",
    "      return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1751128441226,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "42btCopr65EJ",
    "outputId": "82a5220c-2358-4bea-ddc7-30ac0356c6f9"
   },
   "outputs": [],
   "source": [
    "# Test out TimeSeriesDatasetCustom\n",
    "WINDOW_SIZE = 24 # one day of history\n",
    "FORECAST_HORIZON = 1 # one-hour ahead prediction\n",
    "STRIDE = 1 # overlapping window\n",
    "\n",
    "train_data_custom = TimeSeriesDatasetCustom(df=train_df,\n",
    "                                            window_size=WINDOW_SIZE,\n",
    "                                            forecast_horizon=FORECAST_HORIZON,\n",
    "                                            stride=STRIDE)\n",
    "test_data_custom = TimeSeriesDatasetCustom(df=test_df,\n",
    "                                           window_size=WINDOW_SIZE,\n",
    "                                           forecast_horizon=FORECAST_HORIZON,\n",
    "                                           stride=STRIDE)\n",
    "train_data_custom, test_data_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1751128441243,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "U2x6Lfv3FppG",
    "outputId": "e2deee16-9d26-4157-8ac1-053ee7f72379"
   },
   "outputs": [],
   "source": [
    "# Test out the __len__() method\n",
    "# Print out some info about our dataset\n",
    "print(f\"[INFO] Size of the training dataset: {len(train_data_custom)}\")\n",
    "print(f\"[INFO] Size of the test dataset: {len(test_data_custom)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1751128441256,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "NKrXMABcHj61",
    "outputId": "953318dc-ee52-4cff-f519-b2af0d8be777"
   },
   "outputs": [],
   "source": [
    "# Verify the number of samples (size of the dataset - window_size - forecast_horizon) // stride + 1\n",
    "len(train_data_custom) == (len(train_df) - WINDOW_SIZE - FORECAST_HORIZON) // STRIDE + 1, len(test_data_custom) == (len(test_df) - WINDOW_SIZE - FORECAST_HORIZON) // STRIDE + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1751128441265,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "G-4xlWxfJhGT",
    "outputId": "6e7520d2-fa69-4e41-cafc-a7baae670b11"
   },
   "outputs": [],
   "source": [
    "# Print some more info to test out the __getitem__() method\n",
    "dataset_dict = {\"training\": train_data_custom,\n",
    "                \"test\": test_data_custom}\n",
    "\n",
    "for split, dataset in dataset_dict.items():\n",
    "  print(f\"[INFO] Sample 0 of {split} set:\")\n",
    "  print(f\"Data:\\n{dataset[0][0]}\")\n",
    "  print(f\"Shape: {dataset[0][0].shape}\")\n",
    "  print()\n",
    "  print(f\"Target:\\n{dataset[0][1]}\")\n",
    "  print(f\"Shape: {dataset[0][1].shape}\")\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD_ZzkIJLipz"
   },
   "source": [
    "### 2.2 Turn custom loaded time series data into DataLoader's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1751128441299,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "472Hq2uwMP-7",
    "outputId": "112c0b29-dca2-4953-8c8a-f79c2eb24f8d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup number of workers and batch size\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn the custom pytorch Datasets into DataLoaders\n",
    "train_dataloader_custom = DataLoader(dataset=train_data_custom,\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     num_workers=NUM_WORKERS,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=False)\n",
    "test_dataloader_custom = DataLoader(dataset=test_data_custom,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    num_workers=NUM_WORKERS,\n",
    "                                    shuffle=False, # no need to shuffle the test data\n",
    "                                    drop_last=False)\n",
    "train_dataloader_custom, test_dataloader_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1751128441310,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "ANkSedMDwyh8",
    "outputId": "80e134a5-e37f-4159-caf3-e538769002e4"
   },
   "outputs": [],
   "source": [
    "# Print out some information of the custom dataloaders\n",
    "print(f\"[INFO] Train dataloader (custom) has: {len(train_dataloader_custom)} batches with number of samples: {BATCH_SIZE}\")\n",
    "print(f\"[INFO] Test dataloader (custom) has: {len(test_dataloader_custom)} batches with number of samples: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1751128441501,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "_nuNofVKxnMh",
    "outputId": "3597832b-997f-4cdf-de2a-b0ba2758355d"
   },
   "outputs": [],
   "source": [
    "# Get a sample from the custom train dataloader\n",
    "input_window, target = next(iter(train_dataloader_custom))\n",
    "input_window.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1751128441507,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "QeuoUvy43QcJ",
    "outputId": "1bc092d7-61f1-4520-9917-5d64976f8530"
   },
   "outputs": [],
   "source": [
    "# Calculate the shape of the last batch\n",
    "shape_of_last_batch = (len(train_data_custom) % BATCH_SIZE, 2, WINDOW_SIZE)\n",
    "shape_of_last_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vGCjupNx8us"
   },
   "source": [
    "## 3. Replicating TimeLLM: Overview\n",
    "\n",
    "With the dataloader's ready, we can now proceed to replicate TimeLLM's model architecture. Before we start, it's a good practice to think about the **inputs** and **outputs**.\n",
    "\n",
    "* Inputs - What goes into the model? A pair consisting of:\n",
    "  * Input window: A segment of the time series with length `window_size`\n",
    "    * Shape: `(batch_size, num_channels, window_size)`\n",
    "    * E.g. `(32, 2, 48)` - 32 samples in a batch, 2 time series, window size of 48\n",
    "  * Target values: The ground truth value we want our model to predict\n",
    "    * Shape: `(batch_size, num_channels, forecast_horizon)`\n",
    "    * E.g. `(32, 2, 24)` - 32 samples in a batch, 2 time series, 24 ground truth future values\n",
    "\n",
    "* Outputs - What comes out of the model?\n",
    "  * Predictions for future time steps\n",
    "    * Shape: `(batch_size, forecast_horizon)`\n",
    "    * E.g. `(32, 24)` - 32 samples in a batch, 24 predictions into the future\n",
    "  \n",
    "> Note: The shape of the model outpus doesn't have `num_channels` in the second dimension because our model processes each (univariate) time series subsequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGh3W4gUPJyT"
   },
   "source": [
    "### 3.1 TimeLLM overview: pieces of the puzzle\n",
    "\n",
    "* Section 3 introduction + Figure 2: Paper objectives & model architecture overview\n",
    "* Four components: The main four components which TimeLLM can be broken down into\n",
    "\n",
    "#### The introduction of Section 3 outlines the objectives of this paper and provides an overview of the model architecture. Here are some snippets:\n",
    "\n",
    "* Objective:\n",
    "Our model architecture is depicted in Fig. 2. We focus on reprogramming an embedding-visible language foundation model, such as Llama (Touvron et al., 2023) and GPT-2 (Radford et al., 2019), for general time series forecasting without requiring any fine-tuning of the backbone model. Specifically, we consider the following problem: given a sequence of historical observations $\\mathbf{X} \\in \\mathbb{R}^{N \\times T}$ consisting of $N$ different 1-dimensional variables across $T$ time steps, we aim to reprogram a large language model $f(\\cdot)$ to understand the input time series and accurately forecast the readings at $H$ future time steps, denoted by $\\hat{\\mathbf{Y}} \\in \\mathbb{R}^{N \\times H}$, with the overall objective to minimize the mean squared error between the ground truths $\\mathbf{Y}$ and predictions:\n",
    "$$\n",
    "\\frac{1}{H} \\sum_{h=1}^H \\| \\hat{Y}_h - Y_h \\|_F^2\n",
    "$$\n",
    "\n",
    "* Model architecture overview:\n",
    "Our method encompasses three main components:\n",
    "  1. input transformation\n",
    "  2. a pre-trained and frozen LLM\n",
    "  3. output projection.\n",
    "  \n",
    "  Initially, a multivariate time series is partitioned into $N$ univariate time series, which are subsequently processed independently (Nie et al., 2023). The $i$-th series is denoted as $\\mathbf{X}^{(i)} \\in \\mathbb{R}^{1 \\times T}$ which undergoes normalization, patching, and embedding prior to being reprogrammed with learned text prototypes to align the source and target modalities. Then, we augment the LLM’s time series reasoning ability by prompting it together with reprogrammed patches to generate output representations, which are projected to the final forecasts $\\hat{\\mathbf{Y}}^{(i)} \\in \\mathbb{R}^{1 \\times H}$.\n",
    "\n",
    "#### Figure 2\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/chunyagi/ViTimeLLM/main/figures/timellm_architecture.png\" width=600 alt=\"figure 2 from timellm paper\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5I2eRJ7m3-Xy"
   },
   "source": [
    "### Four components\n",
    "\n",
    "#### Section 3.1 breaks TimeLLM into four components:\n",
    "\n",
    "1. **Input Embedding:**\n",
    "Each input channel $\\mathbf{X}^{(i)}$ is first individually normalized to have zero mean and unit standard deviation via reversible instance normalization (RevIN) in mitigating the time series distribution shift (Kim et al., 2021). Then, we divide $\\mathbf{X}^{(i)}$ into several consecutive overlapped or non-overlapped patches (Nie et al., 2023) with length $L_p$; thus the total number of input patches\n",
    "is $P = \\lfloor (T − L_p ) \\rfloor + 2$, where $S$ denotes the horizontal sliding stride. The underlying motivations $S$\n",
    "are two-fold: (1) better preserving local semantic information by aggregating local information into each patch and (2) serving as tokenization to form a compact sequence of input tokens, reducing\n",
    "computational burdens. Given these patches $\\mathbf{X}_P^{(i)} \\in \\mathbb{R}^{P \\times L_p}$ , we embed them as $\\mathbf{X}_P^{(i)} \\in \\mathbb{R}^{P \\times d_m}$\n",
    "adopting a simple linear layer as the patch embedder to create dimensions $d_m$.\n",
    "\n",
    "In summary, it can be broken down into 3 steps:\n",
    "1. Normalization: zero mean and unit std\n",
    "2. Patchification: patchify the input into $P$ patches, each of length $L_p$\n",
    "3. Embedding: embed the patches by a simple a linear layer to change their dimensions from $L_p$ to $d_m$\n",
    "\n",
    "**Note:** There is a hidden step that is not mentioned above. Before normalization, we need to slide an input window across the time series to create input segments.\n",
    "\n",
    "```python\n",
    "# Input Embedding\n",
    "# 1. Normalisation\n",
    "print(x_input.shape) # (batch_size, num_channels, window_size)\n",
    "mean = x_input.mean(dim=2, keepdim=True)\n",
    "std = x_input.std(dim=2, keepdim=True)\n",
    "x_input_normalized = (x_input - mean) / std\n",
    "\n",
    "# 2./3. Patchification + Embedding\n",
    "# We will use a Conv1d layer to patchify and embed the patches simultaneously\n",
    "patcher = Conv1d(in_channels=num_channels,\n",
    "out_channel=embedding_dim,\n",
    "kernel_size=patch_size,\n",
    "stride=patch_size, # non-overlapping patches\n",
    "padding=0) # no padding\n",
    "\n",
    "# Perform the forward pass\n",
    "x_input_patched = patcher(x_input_normalized)\n",
    "print(x_input_patched.shape) # (batch_size, embedding_dim, num_patches)\n",
    "# Make sure the returned sequence embedding dimensions are in the right order\n",
    "x_input_patched = x_input_patched.permute(0, 2, 1) # (batch_size, num_patches, embedding_dim)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMhf45HmPONS"
   },
   "source": [
    "2. **Patch Reprogramming**: To close this gap, we propose reprogramming $\\hat{\\mathbf{X}}_{P}^{(i)}$ using pre-trained word embeddings $\\mathbf{E} \\in \\mathbb{R}^{V \\times D}$ in the backbone, where $V$ is the vocabulary size. Nevertheless, there is no prior knowledge indicating which source tokens are directly relevant. Thus, simply leveraging $\\mathbf{E}$ will result in large and potentially dense reprogramming space. A simple solution is to maintain a small collection of text prototypes by linearly probing $\\mathbf{E}$, denoted as $\\mathbf{E}^{\\prime} \\in \\mathbb{R}^{V^{\\prime} \\times D}$, where $V^{\\prime} \\ll V$. An illustration is in Fig. 3(a). Text prototypes learn connecting language cues, e.g., \"short up\" (red lines) and \"steady down\" (blue lines), which are then combined to represent the local patch information (e.g., \"short up then down steadily\" for characterizing patch 5) without leaving the space where the language model is pre-trained. This approach is efficient and allows for the adaptive selection of relevant source information. To realize this, we employ a multi-head crossattention layer. Specifically, for each head $k=\\{1, \\cdots, K\\}$, we define query matrices $\\mathbf{Q}_{k}^{(i)}=\\hat{\\mathbf{X}}_{P}^{(i)} \\mathbf{W}_{k}^{Q}$, key matrices $\\mathbf{K}_{k}^{(i)}=\\mathbf{E}^{\\prime} \\mathbf{W}_{k}^{K}$, and value matrices $\\mathbf{V}_{k}^{(i)}=\\mathbf{E}^{\\prime} \\mathbf{W}_{k}^{V}$, where $\\mathbf{W}_{k}^{Q} \\in \\mathbb{R}^{d_{m} \\times d}$ and $\\mathbf{W}_{k}^{K}, \\mathbf{W}_{k}^{V} \\in \\mathbb{R}^{D \\times d}$. Specifically, $D$ is the hidden dimension of the backbone model, and $d=\\left\\lfloor\\frac{d_{m}}{K}\\right\\rfloor$. Then, we have the operation to reprogram time series patches in each attention head defined as:\n",
    "$$\n",
    "\\mathbf{Z}_{k}^{(i)}=\\operatorname{ATTENTION}\\left(\\mathbf{Q}_{k}^{(i)}, \\mathbf{K}_{k}^{(i)}, \\mathbf{V}_{k}^{(i)}\\right)=\\operatorname{SOFTMAX}\\left(\\frac{\\mathbf{Q}_{k}^{(i)} \\mathbf{K}_{k}^{(i) \\top}}{\\sqrt{d_{k}}}\\right) \\mathbf{V}_{k}^{(i)}\n",
    "$$\n",
    "\n",
    "  By aggregating each $\\mathbf{Z}_{k}^{(i)} \\in \\mathbb{R}^{P \\times d}$ in every head, we obtain $\\mathbf{Z}^{(i)} \\in \\mathbb{R}^{P \\times d_{m}}$. This is then linearly projected to align the hidden dimensions with the backbone model, yielding $\\mathbf{O}^{(i)} \\in \\mathbb{R}^{P \\times D}$.\n",
    "\n",
    "#### Figure 3(a)\n",
    "\n",
    "  <img src=\"./figures/patch_reprogramming.png\" width=500 alt=\"figure 3a from timellm paper\">\n",
    "\n",
    "In summary, it can be broken down into 3 steps:\n",
    "1. Create text prototypes $\\mathbf{E'} \\in \\mathbb{R}^{V' \\times D} $ by running $\\mathbf{E} \\in \\mathbb{R}^{V \\times D}$ (the word embeddings in the backbone model) through a linear layer\n",
    "2. Employ a multi-head attention crossattention layer on the time series patches (queries) and text prototypes (keys and values) to align the source and target modalities\n",
    "3. Run the reprogrammed patches $\\mathbf{Z}^{(i)} \\in \\mathbb{R}^{P \\times D}$ through a linear layer to align with the backbone's hidden dimensions $D$.\n",
    "\n",
    "In pseudocode:\n",
    "```python\n",
    "# 1. Create text prototypes\n",
    "linear_layer_for_text_prototypes = nn.Linear(in_features=vocab_size, out_features=text_prototype_size)\n",
    "\n",
    "text_prototypes = linear_layer_for_text_prototypes(llm_word_embeddings)\n",
    "\n",
    "# 2. Reprogramming time series patches with multi-head cross attention with text prototypes\n",
    "reprogrammed_patch_embeddings = multihead_cross_attention(query=patches, key=llm_embeddings, value=llm_embeddings)\n",
    "\n",
    "# 3. Run the reprogrammed patches through a linear layer to align with the llm's hidden dimensions\n",
    "linear_layer_for_llm_hidden_dim = nn.Linear(in_features=patch_embedding_dim, out_features=llm_hidden_dim)\n",
    "reprogrammed_patch_embeddings = linear_layer_for_llm_hidden_dim(reprogrammed_patch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oNAwhTLg-64"
   },
   "source": [
    "3. **Prompt-as-Prefix:** Prompts act as prefixes to enrich the input context and guide the transformation of reprogrammed time series patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7X_pjdJguWh"
   },
   "source": [
    "4. **Output Projection:** Upon packing and feedforwarding the prompt and patch embeddings $\\mathbf{O}^{(i)}$ through the frozen LLM as shown in Fig. 2, we discard the prefixal part and obtain the output representations. Following this, we flatten and linear project them to derive the final forecasts $\\hat{\\mathbf{Y}}^{(i)}$.\n",
    "\n",
    "In pseudocode:\n",
    "```python\n",
    "# Input of the backbone model\n",
    "prefix = [token_0, token_1, ...]\n",
    "reprogammed_patch_embeddings = [patch_0, patch_1, ...]\n",
    "input_embeddings = prefix + reprogrammed_patch_embeddings\n",
    "\n",
    "# Perform forward pass to the LLM\n",
    "output_embeddings = LLM(input_embeddings)\n",
    "\n",
    "# Drop the (contextualised) prefix embeddings\n",
    "output_embeddings = output_embeddings[:, prefix_length:, :] # (batch_size, num_patches, embedding_dim)\n",
    "\n",
    "# flatten and pass them to the final forecast layer\n",
    "output_embeddings_flattened = torch.flatten(output_embeddings, start_dim=1) # (batch_size, num_patches * embedding_dim)\n",
    "\n",
    "# Create the forecast head\n",
    "forecast_layer = nn.Linear(in_features=num_patches*embedding_dim,\n",
    "out_features=forecast_horizen) # number of steps ahead (H)\n",
    "\n",
    "# Pass the embedding to the forecast layer\n",
    "forecast = forecast_layer(out_embeddings_flattened)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nin7y5kWGbMO"
   },
   "source": [
    "## 4. Component 1: Normalise the input windows, patchify them and turn them into patch embeddings\n",
    "\n",
    "Let's remind ourselves of the steps in component 1:\n",
    "\n",
    "> In summary, it can be broken down into 3 steps:\n",
    "1. Normalization: zero mean and unit std\n",
    "2. Patchification: patchify the input into $P$ patches, each of length $L_p$\n",
    "3. Embedding: embed the patches by a simple a linear layer to change their dimensions from $L_p$ to $d_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1R-sgSjDXEyn"
   },
   "source": [
    "### 4.1 Normalise the input windows (RevIN)\n",
    "\n",
    "Before we jump into turning the patch embedding layer into a PyTorch module, we need to create a PyTorch module for RevIN (normalisation/inverse-normalisation).\n",
    "\n",
    "Note: Not only will RevIN normalises the input time series segments (to zero mean and unit variance), but it also allows additional (per channel) scaling and shifting:\n",
    "> $x_\\mathrm{norm} = \\gamma \\cdot\n",
    "(\\frac{x-\\mu}{\\sigma+\\epsilon}) + \\beta$\n",
    "\n",
    "Let's try to set up an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1751128441541,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "HBwGMGtNXhe2",
    "outputId": "807db44e-697e-49fc-f464-c88c4f4bde32"
   },
   "outputs": [],
   "source": [
    "# Create example values\n",
    "batch_size = 2\n",
    "num_channels = 2\n",
    "window_size = 6\n",
    "\n",
    "# Create an example batch of input windows\n",
    "torch.manual_seed(42)\n",
    "example_windows = torch.randint(10, size=(batch_size, num_channels, window_size)).type(torch.float32) # (batch size, num_channels, window_size)\n",
    "print(f\"Example input time window:\\n{example_windows}\\n\")\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = torch.mean(example_windows, dim=2, keepdim=True)\n",
    "std = torch.std(example_windows, dim=2, keepdim=True)\n",
    "print(f\"Example mean:\\n{mean}\\n\")\n",
    "print(f\"Example standard deviation:\\n{std}\")\n",
    "print(f\"Mean/std shape: {mean.shape}\\n\")\n",
    "\n",
    "# Create gamma and beta\n",
    "gamma = torch.randint(10, size=(1, num_channels, 1)) # batch size is set to 1 for broadcasting\n",
    "beta = torch.randint(10, size=(1, num_channels, 1))\n",
    "print(f\"Gamma:\\n{gamma}\")\n",
    "print(f\"Beta:\\n{beta}\\n\")\n",
    "\n",
    "# Normalise the examples\n",
    "normalised_example_windows = (example_windows - mean) / std\n",
    "print(f\"Normalised example input time widow:\\n{normalised_example_windows}\")\n",
    "affine_normalised_example_windows = normalised_example_windows * gamma + beta\n",
    "print(f\"Affine normalised example input window:\\n{affine_normalised_example_windows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CuiSBNnlfm9"
   },
   "source": [
    "### 4.2 Turn RevIN into a PyTorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1751128441542,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "ESsJrVTpisZL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RevIN(nn.Module):\n",
    "  \"\"\"Reversible instance normalisation (\"RevIN\" for short) for time series segments.\n",
    "\n",
    "  Args:\n",
    "    num_channels (int): Number of channels/time series in the dataset.\n",
    "    affine (bool): If True, apply a learnable per-channel scale and bias after normalisation.\n",
    "    eps (float): A small constant added to the denominator for numerical stability (prevents division by zero).\n",
    "\n",
    "  Returns:\n",
    "    (torch.tensor): Normalised (and linear transformed) input time series segments.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               num_channels: int,\n",
    "               affine: bool = False, # whether you want an affine transformation after the standard normalisation or not\n",
    "               eps: float = 1e-5): # added to the denominator to prevent division by zero error\n",
    "    super().__init__()\n",
    "\n",
    "    self.affine = affine\n",
    "    self.eps = eps\n",
    "\n",
    "    if affine:\n",
    "      self.gamma = nn.Parameter(torch.ones(1, num_channels, 1),\n",
    "                                requires_grad=True) # make sure it's learnable\n",
    "      self.beta = nn.Parameter(torch.zeros(1, num_channels, 1),\n",
    "                               requires_grad=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Calculate the mean and variance of the input segments\n",
    "    self.mean = torch.mean(x, dim=2, keepdim=True) # set keepdim to True to keep the second dimension\n",
    "    self.std = torch.std(x, dim=2, keepdim=True)\n",
    "\n",
    "    # Normalise it to have zero mean and unit variance\n",
    "    x_norm = (x - self.mean) / (self.std + self.eps)\n",
    "\n",
    "    # If affine is enabled, learn a linear transformation as well\n",
    "    if self.affine:\n",
    "      x_norm = self.gamma * x_norm + self.beta\n",
    "\n",
    "    return x_norm\n",
    "\n",
    "  # Inverse normalisation to convert the input segments back to their original scale\n",
    "  def inverse(self, x_norm):\n",
    "    if self.affine:\n",
    "      # Reverse the linear transformation process\n",
    "      x_norm = (x_norm - self.beta) / (self.gamma + self.eps)\n",
    "\n",
    "    # Reverse normalisation\n",
    "    x = x_norm * self.std + self.mean\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1751128441555,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "LSoUSK1rjVdp",
    "outputId": "c396b158-a032-4b56-afc0-a105d2a6a40b"
   },
   "outputs": [],
   "source": [
    "example_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1751128441560,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "g2GSfaffh22Y",
    "outputId": "29cf141d-fb54-42fd-9925-2191e774dbf9"
   },
   "outputs": [],
   "source": [
    "# Test the RevIN class\n",
    "# Create an instance of RevIN layer\n",
    "revin_layer = RevIN(num_channels=2,\n",
    "                    affine=True)\n",
    "\n",
    "# Pass example windows through RevIN layer\n",
    "print(f\"Input time series windows:\\n{example_windows}\\n\") # add an extra batch dimension\n",
    "revin_example_windows = revin_layer(example_windows)\n",
    "print(f\"Output normalised time series windows:\\n{revin_example_windows}\\n\")\n",
    "inverse_revin_example_windows = revin_layer.inverse(revin_example_windows)\n",
    "print(f\"Output inverse normalised time series windows:\\n{inverse_revin_example_windows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CRrgRP_meQe"
   },
   "source": [
    "### 4.3 Calculate input and output shapes of the patch embedding layer by hand\n",
    "\n",
    "Batch size is omitted for simplicity here\n",
    "\n",
    "* Input shape: $N \\times W$ (number of channels/time series x window size)\n",
    "* Output shape: $P \\times d_m$ (number of patches x hidden dimension of the patch embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1751128441581,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "wGhU-6uAWg9C",
    "outputId": "a406c77d-3443-41d0-b831-693f2211c2c6"
   },
   "outputs": [],
   "source": [
    "# Create example values\n",
    "input_window_size = 6 # window size (W)\n",
    "num_of_channels = 2 # number of time series in the dataset (N)\n",
    "patch_size = 2 # patch length (L_p)\n",
    "non_overlap_stride = 2 # produce non-overlapping patches\n",
    "overlap_stride = 1 # produce overlapping patches\n",
    "patch_embedding_dim = 768 # patch embeddings dimension (d_m)\n",
    "\n",
    "# Calculate the number of patches (P) with non-overlapping stride (assumed no padding)\n",
    "number_of_non_overlap_patches = (input_window_size - patch_size) // non_overlap_stride + 1\n",
    "print(f\"Number of patches with non-overlapping stride {non_overlap_stride}: {number_of_non_overlap_patches}\")\n",
    "\n",
    "# Calculate the number of pacthes (P) with overlapping stride\n",
    "number_of_overlap_patches = (input_window_size - patch_size) // overlap_stride + 1\n",
    "print(f\"Number of patches with overlapping stride {overlap_stride}: {number_of_overlap_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1751128441588,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "12tuwpIJs9m1",
    "outputId": "b50ab36a-6e06-471c-c815-ff48d604f29a"
   },
   "outputs": [],
   "source": [
    "# Input shape\n",
    "patch_embedding_layer_input_shape = (num_of_channels, input_window_size)\n",
    "\n",
    "# Output shape (assumed non-overlapping stride)\n",
    "patch_embedding_layer_output_shape = (number_of_non_overlap_patches, patch_embedding_dim)\n",
    "\n",
    "print(f\"Input shape ({num_of_channels} input windows): {patch_embedding_layer_input_shape}\")\n",
    "print(f\"Output shape (1 sequence of non-overlapping patches): {patch_embedding_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMRUDw7xwQN_"
   },
   "source": [
    "### 4.4  Creating time series patches and turning them into patch embeddings\n",
    "\n",
    "> Note:\n",
    "The embedding method used in this implementation is probably different from the one stated in the paper. In the paper, a multivariate time series is first **partitioned into N univariate time series, which are subsequently processed independently.** But here we used a 1D convolutional layer to embed the information across all the time series **at once** into the patch embeddings. This may require a higher hidden dimension ($d_m$) to capture to capture more information from the other time series (therefore higher compute cost), but it streamlines the data flow pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1751128441591,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "EkndoV9rsF2g",
    "outputId": "b74b3bde-60f2-4724-da18-d4ceaeb16df9"
   },
   "outputs": [],
   "source": [
    "# Create a conv1d layer to turn the time series input window into patches of learnable embeddings (patchification and embedding at the same time)\n",
    "from torch import nn\n",
    "\n",
    "# Set up the patch size (L_p)\n",
    "patch_size = 2\n",
    "\n",
    "conv1d = nn.Conv1d(in_channels=2, # number of time series\n",
    "                   out_channels=768, # patch embedding dimension\n",
    "                   kernel_size=patch_size,\n",
    "                   stride=patch_size, # non-overlapping patches\n",
    "                   padding=0) # no padding\n",
    "conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1751128441594,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "14HL4J1wuCsW",
    "outputId": "32916401-7695-4f47-d670-bbc7f2055e18"
   },
   "outputs": [],
   "source": [
    "# Get a training sample\n",
    "input_window = train_data_custom[0][0]\n",
    "\n",
    "# Remind ourselves of what the data looks like\n",
    "print(f\"[INFO] Input data:\\n{input_window}\") # take the first batch\n",
    "print(f\"[INFO] Shape: {input_window.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1751128441598,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "2w-YXVjATyRs",
    "outputId": "e99d8cd7-7275-4e1f-9ce8-8fe81b0ad2b7"
   },
   "outputs": [],
   "source": [
    "# Pass the input window through the convolutional layer\n",
    "input_window_out_of_conv = conv1d(input_window.unsqueeze(dim=0)) # add batch dimension\n",
    "input_window_out_of_conv, input_window_out_of_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1751128441605,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "OverHjUNWKpS",
    "outputId": "8fa440dc-8a87-4dc6-9299-660c3ced0d93"
   },
   "outputs": [],
   "source": [
    "input_window_out_of_conv.requires_grad # make sure it's learnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYml2m96Y8IY"
   },
   "source": [
    "Let's interpret the shape of the output tensor. 1 is the batch size, 768 means, for each patch in the 1x12 grid, it has its own embeddings of 768 dimensions.\n",
    "\n",
    "Let's plot some random dimensions of the patch embeddings and see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1751128441771,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "GhtgTa8iWunG",
    "outputId": "796d5a0d-cd4c-46b8-aba4-852f3b82d9d8"
   },
   "outputs": [],
   "source": [
    "# Plot random convolutional feature maps (embeddings)\n",
    "import random\n",
    "\n",
    "# Set seed\n",
    "random.seed(42)\n",
    "\n",
    "# Draw 5 random embedding dimensions\n",
    "random_indexes = random.sample(range(0, 768), k=5)\n",
    "print(f\"Showing random convolutional feature maps from indexes {random_indexes}\")\n",
    "\n",
    "# Create plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(7, 7))\n",
    "\n",
    "# Plot random input window feature maps\n",
    "for i, idx in enumerate(random_indexes):\n",
    "  input_window_feature_map = input_window_out_of_conv[:, idx, :]\n",
    "  axs[i].imshow(input_window_feature_map.detach().numpy()) # remove gradient tracking and convert it to numpy array\n",
    "  axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1751128441788,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "A6RRNrgsY3ix",
    "outputId": "d890f94f-2052-4cec-8eca-48aee4cca891"
   },
   "outputs": [],
   "source": [
    "# Get a single feature map in tensor form\n",
    "single_feature_map = input_window_out_of_conv[:, 654, :]\n",
    "single_feature_map, single_feature_map.shape # brighter color means the embedding value is larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2HgZ4UpdRC8"
   },
   "source": [
    "### 4.5 Permute the patch embeddings to make sure the embedding dimension is in the right order\n",
    "\n",
    "Right now, the shape of the output tensor from the conv1d layer is `(1, 768, 3)`, which is not what we want. We want the embedding dimension to be the last dimension: `(1, 3, 768)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1751128441797,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "2CChg4gkebEU",
    "outputId": "f9ef8317-8a34-436e-dee9-d234a74a5f1d"
   },
   "outputs": [],
   "source": [
    "# Rearrange the dimension of the output tensor from conv1d\n",
    "patch_embeddings = input_window_out_of_conv.permute(0, 2, 1)\n",
    "patch_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVIY7c5TmqKS"
   },
   "source": [
    "### 4.6 Turning the patch embedding layer into a PyTorch module\n",
    "\n",
    "We want this module to do a few things:\n",
    "1. Create a layer to turn time series segments into embedding patches using `nn.Conv1d()`\n",
    "2. Permute the patch embeddings dimension, i.e. `(batch_size, num_patches, embedding_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1751128441827,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "JFyo771anKPr"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "  \"\"\"\n",
    "  Create (multivariate) time series patches and embed the patches.\n",
    "\n",
    "  Args:\n",
    "    num_channels (int): Number of time series in the dataset.\n",
    "    patch_size (int): Length of the time series patches.\n",
    "    stride (int): Step size for sliding the kernel across the time series.\n",
    "    embedding_dim (int): Patch Embedding dimensions.\n",
    "    padding (int): Number of paddings added to the beginning and the end of the input time series windows.\n",
    "\n",
    "  Returns:\n",
    "    (torch.tensor) Time series patch embeddings\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               num_channels: int, # number of time series in the dataset (N)\n",
    "               patch_size: int, # patch length (L_p)\n",
    "               stride, # number of steps the kernel moves along the input window\n",
    "               embedding_dim: int = 16, # embedding dimension of the patch embeddings\n",
    "               padding: int = 0): # paddings added to both sides of the input window before convolutions\n",
    "    super().__init__()\n",
    "\n",
    "    # Create a layer to turn a time series input window into embedded patches\n",
    "    self.patcher = nn.Conv1d(in_channels=num_channels,\n",
    "                             out_channels=embedding_dim,\n",
    "                             kernel_size=patch_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding)\n",
    "\n",
    "    self.patch_size = patch_size\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Check if patch size is compatible with the input window size and stride\n",
    "    input_window_size = x.shape[-1]\n",
    "    if (input_window_size + 2*self.padding - self.patch_size) % self.stride != 0:\n",
    "      print(\"Warning: Input window size is incompatible with the given patch size and stride. The last few time steps will be dropped.\") # can turn this into a warning\n",
    "\n",
    "    # Patchify the input windows and embed the patches\n",
    "    x_patched = self.patcher(x)\n",
    "\n",
    "    # Rearrange the dimension of the patches\n",
    "    return x_patched.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUC3heY1iZZC"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "The `padding` parameter in `PatchEmbedding` serves two purposes:\n",
    "1. Allows early and late steps to participate in more patches (get convolved a few more times), improving representations at boundaries.\n",
    "2. Prevents truncation by making the input window size `L_p` compatibe with the given patch size `P` and stride `S`.\n",
    "\n",
    "However, it doesn't support asymmetric padding right now (can work on it later with `F.pad`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1751128441838,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "AVXYSAm3lJIS",
    "outputId": "a37f9e2e-18bd-4db3-c66e-a18316499bc0"
   },
   "outputs": [],
   "source": [
    "input_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1751128441846,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "2RdT534ykMkq",
    "outputId": "bee6bc33-61ca-4fb1-a0ca-c8663652e780"
   },
   "outputs": [],
   "source": [
    "# Test the PatchEmbedding class\n",
    "\n",
    "# Add an extra batch dimension to input_window (the first sample in training dataloader)\n",
    "batchified_input_window = input_window.unsqueeze(dim=0)\n",
    "\n",
    "# Create an instance of a non-overlapping patch embedding layer\n",
    "non_overlap_patchify = PatchEmbedding(num_channels=2,\n",
    "                                      patch_size=2,\n",
    "                                      stride=2,\n",
    "                                      embedding_dim=768,\n",
    "                                      padding=0)\n",
    "\n",
    "# Pass a training example through patch embedding layer\n",
    "print(f\"Input sample size: {batchified_input_window.shape}\")\n",
    "patch_embedded_input_window = non_overlap_patchify(batchified_input_window)\n",
    "print(f\"Output patch embeddings shape: {patch_embedded_input_window.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1751128441850,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "Lbp_r9YhlrWz",
    "outputId": "73473336-67a7-46e6-ef62-38c7922796a1"
   },
   "outputs": [],
   "source": [
    "# Create an instance of an overlapping patch embedding layer\n",
    "overlap_patchify = PatchEmbedding(num_channels=2,\n",
    "                                  patch_size=2,\n",
    "                                  stride=1, # overlapping patch\n",
    "                                  embedding_dim=768,\n",
    "                                  padding=0)\n",
    "\n",
    "# Pass a training example through patch embedding layer\n",
    "print(f\"Input sample size: {batchified_input_window.shape}\")\n",
    "patch_embedded_input_window = overlap_patchify(batchified_input_window)\n",
    "print(f\"Output patch embeddings shape: {patch_embedded_input_window.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1751128441908,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "B5IudDUbn5uD",
    "outputId": "945b8beb-93ec-4abb-ac18-de06716e1d03"
   },
   "outputs": [],
   "source": [
    "# Create an example training sample to show how paddings can help prevent truncation\n",
    "example_long_windows = torch.randint(10, size=(2, 2, 26)).type(torch.float32)\n",
    "\n",
    "# Create an instance of a patch embedding layer with no padding\n",
    "no_pad_patchify = PatchEmbedding(num_channels=2,\n",
    "                                 patch_size=7,\n",
    "                                 stride=7,\n",
    "                                 embedding_dim=768,\n",
    "                                 padding=0) # no padding\n",
    "\n",
    "print(f\"Input sample size: {example_long_windows.shape}\")\n",
    "patch_embedded_example_long_windows_no_pad = no_pad_patchify(example_long_windows)\n",
    "print(f\"Output patch embeddings shape before padding: {patch_embedded_example_long_windows_no_pad.shape}\")\n",
    "\n",
    "# Create an instance of a patch embedding layer with padding\n",
    "padded_patchify = PatchEmbedding(num_channels=2,\n",
    "                                 patch_size=7,\n",
    "                                 stride=7,\n",
    "                                 embedding_dim=768,\n",
    "                                 padding=1) # add padding to the beginning and the end of the input\n",
    "patch_embedded_example_long_windows_with_pad = padded_patchify(example_long_windows)\n",
    "print(f\"Output patch embeddings shape after padding: {patch_embedded_example_long_windows_with_pad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaOtPUs9pwRB"
   },
   "source": [
    "## Component 2: Patch Reprogramming\n",
    "\n",
    "To reprogram the time-series patches, we need access to LLaMA’s pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF1s9GJpxfgW"
   },
   "source": [
    "### Create a function to load a pretrained tokenizer, model and its embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1751128441917,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "Jtwttxr5BsE2"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Dict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def load_llm_backbone(checkpoint: str = \"huggyllama/llama-7b\", # use llama-7b by default\n",
    "                      use_fast: bool = True, # enables fast implementation, but requires Rust installed\n",
    "                      padding_side: str = \"left\", # the side where padding starts\n",
    "                      torch_dtype: torch.dtype = torch.float16, # model weights datatype\n",
    "                      device_map: Optional[Union[str, Dict[str, str]]] = None, # don't let accelerate decide where to put the shards automatically\n",
    "                      low_cpu_mem_usage: bool = True): # avoid ever holding the entire fp32 model (with casted copy) in cpu ram\n",
    "                      # device: torch.device = DEVICE): # where to move the model to if device_map is set to None\n",
    "  \"\"\"\n",
    "  Loads a pretrained tokenizer, model and its embedding layer from Hugging Face transformers.\n",
    "\n",
    "  Args:\n",
    "    checkpoint (str): The model checkpoint you want to load.\n",
    "    use_fast (bool): If set to True, it will enable fast implementation for the tokenizer. Note: It requires Rust to be installed.\n",
    "    padding_side (str): The side where padding starts.\n",
    "    torch_dtype (torch.dtype): Model weights datatype in which you want to load.\n",
    "    device_map (Optional[Union[Dict[str, str]]]): It tells Accelerate where to put each checkpoint shard (GPU or CPU offload).\n",
    "    low_cpu_mem_usage (bool): If set to True, it enables the empty-weights mechanism (creating meta tensors instead of making a full\n",
    "      fp32 copy in CPU RAM).\n",
    "    device (torch.device): The fallback device to move the model to if `device_map` is `None`.\n",
    "\n",
    "  Returns:\n",
    "    Pretrained tokenizer, model and its embedding layer.\n",
    "  \"\"\"\n",
    "  # Load the pretrained tokenizer\n",
    "  tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint,\n",
    "                                            use_fast=use_fast)\n",
    "\n",
    "  # Check if there's a pad token in the tokenizer to enable batch processing\n",
    "  if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # use the <EOS> token for padding if there is no padding token\n",
    "    tokenizer.padding_side = padding_side # \"left\" for autoregressive models\n",
    "\n",
    "  # Load the pretrained model\n",
    "  model = AutoModel.from_pretrained(\n",
    "      pretrained_model_name_or_path=checkpoint,\n",
    "      torch_dtype=torch_dtype,\n",
    "      device_map=device_map,\n",
    "      low_cpu_mem_usage=low_cpu_mem_usage\n",
    "  )\n",
    "\n",
    "  # Set the model to eval mode\n",
    "  model.eval()\n",
    "\n",
    "  # Freeze all of the model's layers\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # Move the model to the target device manually if there is no input device map\n",
    "  # if device_map is None:\n",
    "  #   model.to(device)\n",
    "\n",
    "  # Get the model's embedding layer\n",
    "  embedding_layer = model.get_input_embeddings()\n",
    "\n",
    "  return tokenizer, model, embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638,
     "referenced_widgets": [
      "a5e31316eab44e278d62393c0f104e35",
      "6471fb92e98a4d2aaa17b81c852bb441",
      "1e31deefd9b24abf994a759c77ddc5b4",
      "1c57b3b795354e029670864ebf6488e4",
      "43233ce5e7da424fa1e17589292ea2b2",
      "4eb398534fb947a5ba0ac3e1ea73435a",
      "f2058e6e350b40edadfab315544ef0b4",
      "98e2dd4f64d34419870210bc4413363a",
      "762639f6009542a2806081c94d7de200",
      "8097e73fbca441629f564666b219987a",
      "e231d5bf5f50462db44bf24a007b7d88",
      "c8e79a78e16f4b2a9e8c58e2d30781a0",
      "de080f8d514640aab913d45cac15d3cc",
      "f328a19310884a02848d6777b80cea4d",
      "32a5bef958ef436798264c28b13bec98",
      "4215153bfc46411e83c804334ac9d5d2",
      "27ec0f7a9d244db5befa33401a02943a",
      "6a4baa6afbe546fc998ac31f3b25dacd",
      "1ce181f258cc47e29eb64b8ccb3de794",
      "9d7af3add0984a36b2478c87c8adb7df",
      "9fb5aa9657714d19b18a5c0ab3d93478",
      "86ab795d2a57485ab0a4e2a6792b0836",
      "127f6d5f72e1477dbbe01ff480fe9e22",
      "c255a43be62d4b9ba145e764e060a950",
      "12033af79f5f4fc5bf58186926f03e81",
      "89c320ae30e548e6afe20e1fe919f7da",
      "11c7e3d5ae5a4af98859c98848c5e497",
      "ab9514081130454988271b26ba799bed",
      "3a0c7315287448f991ae7ce5aa150942",
      "4560aa9a160e444aa6fee952f7e4e0a9",
      "23019d8ed0554ebb93e077353c7437ed",
      "367f3242d4b0415fa604773d7d1b2622",
      "1022228c36174c238b48c3cddd7d685f",
      "9f0a3a6079af440e942bed23a17c7930",
      "14be3655a8354b4196105eaec09a7f4c",
      "470063248956448c9ddb684c3be40492",
      "8872ffec5de5448ca881755f604c6c31",
      "290bd5ff6132455abdbf1fbb47de4350",
      "634dc4558b7c4b409d673552c0090e64",
      "6d8f26c98e8c4841bfcd381abb68c5b5",
      "794e83002a19418e9bb30e4f69cdea01",
      "e4fcfafb2e744a31be57380769fdd68e",
      "7d129bcb99e74716b4e5fcfb6323ceab",
      "1d431a96af7743b2bfe87df92dc14a3b",
      "f868b62f00e741a28f0616595cbfe8b5",
      "3430064a87c8447394c3a624097d4df2",
      "bfbda13351524fd097ff94991a125209",
      "fb6fa72b087a4202ad189d45ffbb5a70",
      "0b6cc08114e54699bacb07ba22916760",
      "beebd466ef78475196d8107d8cd08be6",
      "2199365465ba4fe895272418fe9b37a0",
      "fe54249c1b8241e4ac8a5064a53d6edb",
      "8ac7c5155c6b403b9a46434304996578",
      "adae7f4803654e96b76d0eed8ea041c3",
      "926b51c9313c4b67bb124b784f2cf470",
      "0ba4c8dfb41e42b3a2c7089f3a682459",
      "7ae1513f19a74f158a9618a022cc1271",
      "9f97dea8485841ca8807d54c94bfa7a2",
      "3de485a06c1c4f4baf9a670c97adef56",
      "f3a836447b5b4ceca79dea21a8c015b7",
      "9e57099784374980b832fffd1c5ff992",
      "2a66cdc322344b81820749e4bd9fe497",
      "11bea1d5532d401981608d77cb152991",
      "32782a84bdb741218aa607fc67634da7",
      "0fdfadfe6891437eab6cbf4b17ff97ca",
      "492f47725fb24915a37f642f97f0b836"
     ]
    },
    "executionInfo": {
     "elapsed": 18826,
     "status": "ok",
     "timestamp": 1751128460744,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "JoPVteRfICV_",
    "outputId": "38109c7c-ecf1-4d77-8985-bcece9a000da"
   },
   "outputs": [],
   "source": [
    "# Test the function\n",
    "tokenizer, llm_model, llm_embedding_layer = load_llm_backbone(checkpoint=\"distilgpt2\",\n",
    "                                                              torch_dtype=torch.float32)\n",
    "# Move the model to the gpu\n",
    "llm_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1751128461233,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "sLrkOOAkJl_g",
    "outputId": "b0d11315-f03e-4090-e32f-b4d265154cb3"
   },
   "outputs": [],
   "source": [
    "# Inspect the llm's pretrained embedding layer\n",
    "print(f\"[INFO] LLM's pretrained embedding layer: {llm_embedding_layer.weight}\")\n",
    "print(f\"[INFO] Shape of the LLM's pretrained embedding layer: {llm_embedding_layer.weight.shape}\")\n",
    "print(f\"[INFO] Datatype of the LLM's pretrained embedding layer: {llm_embedding_layer.weight.dtype}\")\n",
    "print(f\"[INFO] The pretrained embedding layer has {llm_embedding_layer.weight.shape[0]} vocabs and each of the embedding has {llm_embedding_layer.weight.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DneCXf5NXdL5"
   },
   "source": [
    "Now we can move on to the patch reprogramming layer:\n",
    "\n",
    "Steps:\n",
    "1. Pass the LLM's pretrained embedding layer through a linear layer to learn text prototypes (from vocab size $V$ to text prototype size $V'$, where $V' << V$)\n",
    "3. Pass the text prototypes through another linear layer to project them from hidden size $D$ to the patch embedding dimension $d_{m}$\n",
    "4. Cross attention between the patch embeddings (queries) and the learnt text prototypes (keys and values)\n",
    "5. Run the reprogrammed patch embeddings through a final linear layer to align with the LLM's hidden dimension $D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1751128461237,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "2wjkxst2M8nM"
   },
   "outputs": [],
   "source": [
    "class PatchReprogramming(nn.Module):\n",
    "  \"\"\"\n",
    "  Reprogram the time series patch embeddings with text prototypes before feeding\n",
    "  them into the frozen LLM to align the two different modalities (time series and text).\n",
    "\n",
    "  Args:\n",
    "    pretrained_word_embeddings (torch.nn.modules.sparse.Embedding): Backbone model's pretrained word embeddings.\n",
    "    num_heads (int): Number of attention heads.\n",
    "    vocab_size (int): Number of vocabularies in the LLM's tokenizer.\n",
    "    text_prototype_size (int): Number of text prototypes.\n",
    "    hidden_size (int): Hidden dimension of the word embeddings used in the frozen LLM (D).\n",
    "    embedding_dim (int): Embedding dimension of the patch embeddings (d_m).\n",
    "    attn_dropout (float): Dropout layer used in the multi-head attention layer.\n",
    "\n",
    "  Returns:\n",
    "    (torch.tensor) Reprogrammed time series patch embeddings with hidden size\n",
    "    aligned with the frozen LLM (D).\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               pretrained_word_embeddings: torch.nn.modules.sparse.Embedding, # backbone model's pretrained word embeddings\n",
    "               num_heads: int, # number of attention heads\n",
    "               vocab_size: int = 500, # number of vocabs\n",
    "               text_prototype_size: int = 100, # number of text prototypes\n",
    "               hidden_size: int = 512, # hidden size of pretrained word embeddings\n",
    "               embedding_dim: int = 768, # embedding dimension of patch embeddings\n",
    "               attn_dropout: float = 0): # dropout layer used in attention\n",
    "    super().__init__()\n",
    "\n",
    "    self.pretrained_word_embeddings = pretrained_word_embeddings\n",
    "\n",
    "    # Create a linear layer to learn text prototypes from the pretrained word embeddings (V -> V')\n",
    "    self.text_prototype_linear_layer = nn.Linear(in_features=vocab_size,\n",
    "                                                 out_features=text_prototype_size)\n",
    "\n",
    "    # Create a linear layer to project the text prototypes to the same dimension as the patch embeddings (D -> d_m)\n",
    "    self.text_prototype_to_patch_embedding_layer = nn.Linear(in_features=hidden_size,\n",
    "                                                             out_features=embedding_dim)\n",
    "\n",
    "    # Create multi-head cross-attention layer\n",
    "    self.cross_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                            num_heads=num_heads,\n",
    "                                            dropout=attn_dropout,\n",
    "                                            batch_first=True) # use batch size as the first dimension (batch_size, num_patches, embedding_dim)\n",
    "\n",
    "    # Create a linear layer to align the reprogrammed patch embeddings with the LLM's hidden size (d_m -> D)\n",
    "    self.patch_embedding_to_llm_token_layer = nn.Linear(in_features=embedding_dim,\n",
    "                                                        out_features=hidden_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Get the weights of the pretrained word embeddings and add an extra batch size dimension\n",
    "    pretrained_word_embeddings = self.pretrained_word_embeddings.weight.unsqueeze(dim=0).expand(x.shape[0], -1, -1) # shape: (batch_size, vocab_size, hidden_size)\n",
    "\n",
    "    # Learn text prototypes\n",
    "    text_prototypes = self.text_prototype_linear_layer(pretrained_word_embeddings.permute(0, 2, 1)) # permute the dimensions of pretrained word embeddings so that vocab size is last: (batch_size, hidden_size, vocab_size)\n",
    "\n",
    "    # Project them onto the same dimension that the patch embeddings are living in\n",
    "    text_prototypes = self.text_prototype_to_patch_embedding_layer(text_prototypes.permute(0, 2, 1)) # permute the dimensions again so that the hidden size is last: (batch_size, vocab_size, hidden_size)\n",
    "\n",
    "    # Cross-attention between text prototypes and patch embeddings\n",
    "    attn_output, _ = self.cross_attn(query=x,\n",
    "                                     key=text_prototypes,\n",
    "                                     value=text_prototypes,\n",
    "                                     need_weights=False)\n",
    "\n",
    "    # Project the attention output (reprogrammed patch embeddings) to the LLM's hidden size\n",
    "    return self.patch_embedding_to_llm_token_layer(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1751128461495,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "osmoQvJlH4WQ",
    "outputId": "91059776-f69f-4e55-e5b7-22814ac6daf2"
   },
   "outputs": [],
   "source": [
    "# Test if it works\n",
    "print(f\"Patch embeddings shape: {patch_embeddings.shape}\\n\")\n",
    "\n",
    "# Create an instance of PatchReprogamming layer\n",
    "patch_reprogramming_layer = PatchReprogramming(pretrained_word_embeddings=llm_embedding_layer,\n",
    "                                               num_heads=12, # number of attention heads\n",
    "                                               vocab_size=llm_embedding_layer.weight.shape[0], # vocab size of the backbone model\n",
    "                                               hidden_size=llm_embedding_layer.weight.shape[1]) # hidden/embedding dimension used in the backbone model\n",
    "\n",
    "# Move the patch reprogramming layer to gpu because our pretrained word embeddings live in the gpu\n",
    "patch_reprogramming_layer.to(DEVICE)\n",
    "\n",
    "# Reprogram the patch embeddings\n",
    "reprogrammed_patch_embeddings = patch_reprogramming_layer(patch_embeddings.to(DEVICE)) # move patch embeddings to the gpu as well\n",
    "print(f\"Reprogrammed patch embeddings:\\n{reprogrammed_patch_embeddings}\")\n",
    "print(f\"Shape (batch_size, num_patches, embedding_dim) -> (batch_size, num_patches, hidden_size): {reprogrammed_patch_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NVUvevvxmA5"
   },
   "source": [
    "## Component 3: Prompt-as-Prefix\n",
    "\n",
    "Now with the tokenizer loaded from Hugging Face, we can try to write some prompt as prefix to the LLM.\n",
    "\n",
    "Figure 4 gives us a prompt example:  \n",
    "\n",
    "#### Figure 4\n",
    "\n",
    "  <img src=\"./figures/example_prompt.png\" width=500 alt=\"figure 4 from timellm paper\">\n",
    "\n",
    "> Note: For now, we assume the prompt to be **static** (same prompt will be reused across all batches) for simplicity. But later we can also explore **dynamic** prompts (varied across batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1751128667548,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "dFFftS-Psesy",
    "outputId": "5c033896-ed1c-410e-d647-eb968523f03b"
   },
   "outputs": [],
   "source": [
    "# Setup window size and forecast horizon\n",
    "WINDOW_SIZE = 24\n",
    "FORECAST_HORIZON = 1\n",
    "\n",
    "# Create prompt as prefix to the LLM\n",
    "prompt_as_prefix = f\"\"\"This toy multivariate time series dataset is designed to validate the TimeLLM architecture. \\\n",
    "It has two channels - standard sine and cosine waves with additive Gaussian noise (as the sampling noise), \\\n",
    "sampled hourly over five weeks (840 points total, spanning 5 full 0 to 2pi cycles). The dataset is split into: \\\n",
    "- Training: 4 weekly cycles (80% of data, 672 points)\n",
    "- Test: 1 weekly cycle (20% of data, 168 points).\n",
    "Below is the information about the input time series:\n",
    "\n",
    "[BEGIN DATA]\n",
    "***\n",
    "[DOMAIN]: Both channels are generated by standard sine and cosine functions, with additive Gaussian noise of mean 0 \\\n",
    "and standard deviation {SIGMA}. Leverage their 2pi-periodicity to inform your forecast.\n",
    "***\n",
    "[INSTRUCTION]: Predict the next {FORECAST_HORIZON} steps given the previous {WINDOW_SIZE} steps information {(WINDOW_SIZE//24)} day(s) attached. \\\n",
    "Focus on learning the dominant periodic patterns while being robust to small deviations.\n",
    "***\n",
    "[END DATA]\"\"\"\n",
    "prompt_as_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1751128667550,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "g7EmNmXvydIX"
   },
   "outputs": [],
   "source": [
    "# Pass the prompt as prefix to the tokenizer\n",
    "# tokenizer(prompt_as_prefix, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1751128667556,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "c5EDjqNXLjzb",
    "outputId": "4ba74982-be1f-4dff-e62d-0402481dd2ac"
   },
   "outputs": [],
   "source": [
    "# Get the maximum sequence length the tokenizer can handle\n",
    "tokenizer_max_input_sequence_length = tokenizer.model_max_length\n",
    "print(f\"[INFO] Tokenizer maximum sequence length: {tokenizer_max_input_sequence_length}\")\n",
    "\n",
    "# Get the actual length of the prompt\n",
    "num_of_tokens_used_in_prompt = len(tokenizer(prompt_as_prefix).input_ids)\n",
    "print(f\"Number of tokens used in the prompt: {num_of_tokens_used_in_prompt}\")\n",
    "print(f\"Number of available tokens for the time series patches: {tokenizer_max_input_sequence_length - num_of_tokens_used_in_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZNUo8SW-Bx8"
   },
   "source": [
    "**Note:** In general, one word doesn't correspond to one token, so it's important to be keep track of the number of tokens used in the prompt-as-prefix part and the maximum number of tokens the tokenizer can take at a time (context window size) or it might lead to truncation of the time series patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1751128667562,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "NRbu9QGKX2Xf",
    "outputId": "c877c334-25cf-467a-dba9-e8b760c01ad4"
   },
   "outputs": [],
   "source": [
    "# Test out the truncation behaviour of the tokenizer\n",
    "num_of_patches = 900 # number of time series patches\n",
    "max_length = tokenizer_max_input_sequence_length - num_of_patches\n",
    "print(f\"Number of available tokens for the prompt: {max_length}\")\n",
    "\n",
    "tokenized_prompt = tokenizer(prompt_as_prefix,\n",
    "                             truncation=True, # enables truncation\n",
    "                             max_length=max_length) # sets upper limit for the prefixal prompt tokens\n",
    "print(f\"Number of tokens used in the prompt after truncation: {len(tokenized_prompt.input_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkx4lMrJ1-L_"
   },
   "source": [
    "### Prepend the reprogrammed time series patch embeddings with the prefixal prompt and then pass them to the frozen LLM.\n",
    "\n",
    "Steps:\n",
    "1. tokenize the prefix and turn them into embeddings with the forward method of `llm_embedding_layer`\n",
    "2. Create an attention mask of all 1's for the reprogrammed patch embeddings (no paddings were used)\n",
    "3. Concatenate the prefix embeddings with the reprogrammed patch embeddings\n",
    "4. Concatenate the prefix attention mask with the reprogrammed patch embeddings' attention mask\n",
    "5. Pass the concatenated embeddings and attention masks into the frozen LLM\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1751128667567,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "wTUImi466J4k",
    "outputId": "5facb338-7e67-4d76-a1f9-4cfd415286b1"
   },
   "outputs": [],
   "source": [
    "# 1. Tokenize the prefixal prompt\n",
    "tokenized_prompt_as_prefix = tokenizer(prompt_as_prefix,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       truncation=None, # if set to True, it will truncate the tokens exceeding the maximum length specified below\n",
    "                                       max_length=None) # can be used to limit the length of prefix tokens\n",
    "\n",
    "prefix_input_ids = tokenized_prompt_as_prefix.input_ids\n",
    "prefix_attention_mask = tokenized_prompt_as_prefix.attention_mask\n",
    "print(f\"Input (token) ids of prefix: {prefix_input_ids}\")\n",
    "print(f\"Shape of prefix input ids: {prefix_input_ids.shape}\\n\")\n",
    "print(f\"Attention mask of prefix: {prefix_attention_mask}\")\n",
    "print(f\"Shape of prefix attention mask: {prefix_attention_mask.shape}\\n\")\n",
    "\n",
    "# 1.1 Turn the prefix into embeddings\n",
    "prefix_embeddings = llm_embedding_layer(prefix_input_ids.to(DEVICE))\n",
    "print(f\"The llm embedding layer outputs prefix embeddings: {prefix_embeddings}\")\n",
    "print(f\"Shape of prefix embeddings: {prefix_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1751128667573,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "tPAxnxlhZLqZ",
    "outputId": "9d42eaa6-8173-429b-9c55-9d6e8159fe79"
   },
   "outputs": [],
   "source": [
    "# Check if the LLM's pretrained embedding layer and the prefix embeddings are frozen\n",
    "llm_embedding_layer.weight.requires_grad, prefix_embeddings.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1751128667634,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "52bdxc8XTPR1",
    "outputId": "97e15f0c-f38d-4ddd-a370-4790c1469a4e"
   },
   "outputs": [],
   "source": [
    "reprogrammed_patch_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1751128667641,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "Nq2gBJap-uRd",
    "outputId": "d33c3b1a-624a-4366-87d2-40ee2662e469"
   },
   "outputs": [],
   "source": [
    "# 2. Create an attention mask of all ones for the reprogrammed patch embeddings\n",
    "reprogrammed_patch_attention_mask = torch.ones(1, 12)\n",
    "reprogrammed_patch_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1751128667646,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "l9BDQGGC65mW",
    "outputId": "7884729b-4fc6-46d3-ccea-7acb3cb3b507"
   },
   "outputs": [],
   "source": [
    "# 3. Concatenate the prefix embeddings and the reprogrammed patch embeddings\n",
    "input_embeddings = torch.cat([prefix_embeddings, reprogrammed_patch_embeddings], dim=1)\n",
    "# 4. Concatenate the prefix attention mask and the reprogrammed patch attention mask\n",
    "input_attention_mask = torch.cat([prefix_attention_mask, reprogrammed_patch_attention_mask], dim=1)\n",
    "print(f\"[INFO] Shape of input embeddings (prefix embeddings + reprogrammed patch embeddings): {input_embeddings.shape}\")\n",
    "print(f\"[INFO] Shape of input attention mask (prefix attention mask + reprogrammed patch attention mask): {input_attention_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1751128667650,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "XO5wHxJDTznS",
    "outputId": "1f55fcd8-43f8-4a6c-bada-3c38322c2d24"
   },
   "outputs": [],
   "source": [
    "num_of_tokens_used_in_prompt + reprogrammed_patch_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1751128667655,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "YhT4g7mLV8Fj",
    "outputId": "ac33b426-a40b-4791-e28d-7ba02c9a0736"
   },
   "outputs": [],
   "source": [
    "reprogrammed_patch_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1751128667659,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "W9-HJ5Xf6cdq",
    "outputId": "6f634f74-6e0d-47c0-886a-f0650d1f4326"
   },
   "outputs": [],
   "source": [
    "# 2. Create an attention mask of all ones for the reprogrammed patch embeddings\n",
    "reprogrammed_patch_attention_mask = torch.ones(size=(1, 12),\n",
    "                                               dtype=torch.int64)\n",
    "reprogrammed_patch_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1751128667663,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "AnyvBgiQ6guC",
    "outputId": "b5bcaf53-9585-4192-d9e5-14db17f6b647"
   },
   "outputs": [],
   "source": [
    "# 3. Concatenate the prefix embeddings and the reprogrammed patch embeddings\n",
    "input_embeddings = torch.cat([prefix_embeddings, reprogrammed_patch_embeddings], dim=1)\n",
    "# 4. Concatenate the prefix attention mask and the reprogrammed patch attention mask\n",
    "input_attention_mask = torch.cat([prefix_attention_mask, reprogrammed_patch_attention_mask], dim=1)\n",
    "print(f\"Shape of input embeddings (prefix embeddings + reprogrammed patch embeddings): {input_embeddings.shape}\")\n",
    "print(f\"Shape of input attention mask (prefix attention mask + reprogrammed patch attention mask): {input_attention_mask.shape}\")\n",
    "print(f\"Datatype of input attention mask: {input_attention_mask.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1751128667691,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "PEM-ontG72DM"
   },
   "outputs": [],
   "source": [
    "# Pass the concatenated embeddings and attention mask into the frozen LLM\n",
    "outputs = llm_model(inputs_embeds=input_embeddings.to(DEVICE),\n",
    "                    attention_mask=input_attention_mask.to(DEVICE),\n",
    "                    return_dict=True,\n",
    "                    output_hidden_states=False)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1751128667699,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "qxkXYCWs77ZA",
    "outputId": "f9df16c7-700f-4554-996d-356377cf885f"
   },
   "outputs": [],
   "source": [
    "# Get the output embeddings from the LLM\n",
    "output_embeddings = outputs.last_hidden_state\n",
    "print(f\"[INFO] Output (contextualised) embeddings from the frozen LLM:\\n{output_embeddings}\")\n",
    "print(f\"[INFO] Shape of the output embeddings: {output_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkyPYAE91ZSI"
   },
   "source": [
    "## Component 4: Output Projection\n",
    "\n",
    "After passing the input embeddings (prefix embeddings + reprogrammed patch embeddings) into the LLM, it will output the contextualised embeddings. Then we'll discard the prefixal part, and map the remaining time series output embeddings to obtain forecasts with a linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT7ZgVgCpAvX"
   },
   "source": [
    "## 5. Putting it all together to create TimeLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1751128667701,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "HTAvC8-fphVO"
   },
   "outputs": [],
   "source": [
    "class TimeLLM(nn.Module):\n",
    "  \"\"\"\n",
    "  The TimeLLM architecture replicated from the TimeLLM paper. See the paper: https://arxiv.org/abs/2310.01728.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               num_channels: int,\n",
    "               window_size: int,\n",
    "               forecast_horizon: int,\n",
    "               patch_size: int,\n",
    "               stride: int,\n",
    "               affine: bool = False,\n",
    "               eps: float = 1e-5,\n",
    "               embedding_dim: int = 768,\n",
    "               padding: int = 0,\n",
    "               checkpoint: str = \"huggyllama/llama-7b\",\n",
    "               use_fast: bool = True,\n",
    "               padding_side: str = \"left\",\n",
    "               torch_dtype: torch.dtype = torch.float16,\n",
    "               device_map: Optional[Union[str, Dict[str, str]]] = None,\n",
    "               low_cpu_mem_usage: bool = True,\n",
    "               reprogramming_head: int = 12,\n",
    "               vocab_size: int = 32000,\n",
    "               text_prototype_size: int = 100,\n",
    "               hidden_size: int = 4096,\n",
    "               reprogramming_attn_dropout: float = 0.0,\n",
    "               prompt_as_prefix: Optional[str] = None):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_channels = num_channels\n",
    "    self.forecast_horizon = forecast_horizon\n",
    "\n",
    "    # Calculate the number of patches\n",
    "    self.num_patches = (window_size + 2*padding - patch_size) // stride + 1\n",
    "\n",
    "    # Create a RevIN layer\n",
    "    self.revin = RevIN(num_channels,\n",
    "                       affine=False) # downcast to half precision (fp16) to match the model parameters precision\n",
    "\n",
    "    # Create patch embedding layer\n",
    "    self.patch_embedding_layer = PatchEmbedding(num_channels=num_channels,\n",
    "                                                patch_size=patch_size,\n",
    "                                                stride=stride,\n",
    "                                                embedding_dim=embedding_dim,\n",
    "                                                padding=padding)\n",
    "\n",
    "    # Load the tokenizer, LLM and its embedding layer\n",
    "    self.tokenizer, self.llm_model, self.llm_embedding_layer = load_llm_backbone(checkpoint=checkpoint,\n",
    "                                                                                 use_fast=use_fast,\n",
    "                                                                                 padding_side=padding_side,\n",
    "                                                                                 torch_dtype=torch_dtype,\n",
    "                                                                                 device_map=device_map,\n",
    "                                                                                 low_cpu_mem_usage=low_cpu_mem_usage)\n",
    "\n",
    "    # Create Patch reprogramming layer\n",
    "    self.patch_reprogramming_layer = PatchReprogramming(pretrained_word_embeddings=self.llm_embedding_layer,\n",
    "                                                        num_heads=reprogramming_head,\n",
    "                                                        vocab_size=vocab_size,\n",
    "                                                        text_prototype_size=text_prototype_size,\n",
    "                                                        hidden_size=hidden_size,\n",
    "                                                        embedding_dim=embedding_dim,\n",
    "                                                        attn_dropout=reprogramming_attn_dropout)\n",
    "    self.prompt_as_prefix = prompt_as_prefix\n",
    "\n",
    "    # Tokenize the prompt if there's one\n",
    "    if prompt_as_prefix:\n",
    "      # Calculate the prompt tokens maximum length\n",
    "      max_prompt_length = tokenizer.model_max_length - self.num_patches\n",
    "\n",
    "      # Tokenize the prompt\n",
    "      tokenized_prompt = tokenizer(prompt_as_prefix,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   truncation=True, # enables truncation\n",
    "                                   max_length=tokenizer.model_max_length-self.num_patches) # sex max length for the prompt tokens based on the number of time series patches\n",
    "\n",
    "      # Warn the user if the prompt token length exceeds its maximum\n",
    "      if tokenized_prompt.input_ids.shape[1] > max_prompt_length:\n",
    "        print(f\"[WARNING]: Prompt token length exceeds its maximum length: {max_prompt_length}. The last part of the prompt will be truncated. Consider shortening your prompt.\")\n",
    "\n",
    "      # Turn the prompt tokens into embeddings with the pretrained LLM's embedding layer\n",
    "      prompt_embeddings = self.llm_embedding_layer(tokenized_prompt.input_ids)\n",
    "\n",
    "      # Save the prompt embeddings and the prompt attention mask as buffer\n",
    "      self.register_buffer(\"prompt_embeddings\", prompt_embeddings)\n",
    "      self.register_buffer(\"prompt_attention_mask\", tokenized_prompt.attention_mask)\n",
    "\n",
    "    # Create a flatten layer\n",
    "    self.flatten_layer = nn.Flatten(start_dim=1)\n",
    "\n",
    "    # Create a forecast head\n",
    "    self.forecast_head = nn.Linear(in_features=self.num_patches*hidden_size,\n",
    "                                   out_features=num_channels*forecast_horizon)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # 1. Normalise the input time window\n",
    "    x_norm = self.revin(x)\n",
    "\n",
    "    # 2. Turn them into patch embeddings\n",
    "    patch_embeddings = self.patch_embedding_layer(x_norm)\n",
    "\n",
    "    # 3. Reprogram the patch embeddings\n",
    "    reprogrammed_patch_embeddings = self.patch_reprogramming_layer(patch_embeddings)\n",
    "\n",
    "    # 4. Create attention mask for the reprogrammed patches\n",
    "    reprogrammed_patch_attention_mask = torch.ones(x.shape[0], self.num_patches).to(x.device) # send the reprogrammed attention mask to gpu\n",
    "\n",
    "    # 5. Check if there is a prompt\n",
    "    if self.prompt_as_prefix:\n",
    "      # Concatenate the prompt embeddings and the reprogrammed patch embeddings\n",
    "      input_embeddings = torch.cat([self.prompt_embeddings.expand(x.shape[0], -1, -1), reprogrammed_patch_embeddings], dim=1) # expand the prompt embeddings batch dimension\n",
    "\n",
    "      # Concatenate the attention masks as well\n",
    "      input_attention_mask = torch.cat([self.prompt_attention_mask.expand(x.shape[0], -1), reprogrammed_patch_attention_mask], dim=1)\n",
    "\n",
    "    # If not, use reprogrammed patch embeddings and attention mask as the only input to the LLM\n",
    "    else:\n",
    "      input_embeddings = reprogrammed_patch_embeddings\n",
    "      input_attention_mask = reprogrammed_patch_attention_mask\n",
    "\n",
    "    # 6. Pass the input embeddings and attention mask to the frozen LLM\n",
    "    outputs = self.llm_model(inputs_embeds=input_embeddings,\n",
    "                             attention_mask=input_attention_mask,\n",
    "                             return_dict=True,\n",
    "                             output_hidden_states=False)\n",
    "\n",
    "    # 7. Discard the prefix (if any) and only get the output embeddings for the time series patches\n",
    "    output_embeddings = outputs.last_hidden_state[:, -self.num_patches:, :]\n",
    "\n",
    "    # 8. Flatten the output embeddings\n",
    "    flattened_output_embeddings = self.flatten_layer(output_embeddings)\n",
    "\n",
    "    # 9. Project them to get normalised forecast values\n",
    "    y_norm = self.forecast_head(flattened_output_embeddings).view(x.shape[0], self.num_channels, self.forecast_horizon)\n",
    "\n",
    "    # 10. Return the rescaled forecast values\n",
    "    return self.revin.inverse(y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1751128668335,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "aFAXcuzny-tj"
   },
   "outputs": [],
   "source": [
    "# Test the TimeLLM class\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a random input window tensor\n",
    "random_input_window = torch.randn(1, 2, 24).to(DEVICE)\n",
    "\n",
    "# Create an instance of TimeLLM\n",
    "timellm = TimeLLM(checkpoint=\"distilgpt2\",\n",
    "                  num_channels=2,\n",
    "                  window_size=24,\n",
    "                  forecast_horizon=1,\n",
    "                  patch_size=2,\n",
    "                  stride=2,\n",
    "                  torch_dtype=torch.float32,\n",
    "                  vocab_size=50257,\n",
    "                  hidden_size=768,\n",
    "                  prompt_as_prefix=prompt_as_prefix).to(DEVICE) # send the model to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1751128668345,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "YnH7aOdvgr6U",
    "outputId": "513bd4a4-f34a-46f3-c34c-205d76203432"
   },
   "outputs": [],
   "source": [
    "# Pass the input window tensor to TimeLLM\n",
    "forecast = timellm(random_input_window)\n",
    "print(f\"[INFO] Model output: {forecast} | Shape: {forecast.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1751128668346,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "Nm6ubEeg9Se0"
   },
   "outputs": [],
   "source": [
    "# Get a summary of our TimeLLM model\n",
    "# summary(model=timellm,\n",
    "#         input_size=(1, 2, 24),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS6DKpL69lvJ"
   },
   "source": [
    "## 6. Setting up training code for our custom TimeLLM\n",
    "\n",
    "We've replicated the TimeLLM architecture (with a toy LLM), now let's set up training codes to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FjBHFlMPwg9"
   },
   "source": [
    "### 6.1 Creating an optimizer\n",
    "\n",
    "The paper states it uses the Adam optimizer (section B.4 Model Configurations), but it doesn't mention the values for $B1$ and $B2$, so we'll just stick to the default values for now.\n",
    "\n",
    "The paper uses different learning rates for different datasets. For now, we'll just use `10^-3`, which is used in the LTF-ETTh1 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRNXpW1EP55K"
   },
   "source": [
    "### 6.2 Creating a loss function\n",
    "\n",
    "The paper mentions it uses mean square error (MSE) as its loss function, we can implement it with `torch.nn.MSELoss()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1751128668346,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "rU-ZEyurUmcI"
   },
   "outputs": [],
   "source": [
    "# timellm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1751128668364,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "yAvskd1vS47b"
   },
   "outputs": [],
   "source": [
    "# Setup loss function and optimizer\n",
    "optimizer = torch.optim.Adam(params=timellm.parameters(),\n",
    "                             lr=1e-3)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVp25bBZU1pU"
   },
   "source": [
    "### 6.3 Setup training code\n",
    "\n",
    "Let's create training and testing codes inside a function so that they can be reused in the future.\n",
    "\n",
    "TODO:\n",
    "- Do a three split instead (training-validation-test)\n",
    "- Compile the model\n",
    "- Use AMP\n",
    "- When we use bigger models, e.g. Llama-7B in the future, Use `tf32` instead of `float32` (PyTorch's default) or load the model weights in `float16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1751128668899,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "P0Xfuty9YruT",
    "outputId": "dffbbca0-87c4-4aa2-81c1-cdb6d3c58557"
   },
   "outputs": [],
   "source": [
    "# Get an example from the training dataloader\n",
    "example = next(iter(train_dataloader_custom))\n",
    "\n",
    "# Print out some info of the example\n",
    "print(f\"Shape of training sample: {example[0].shape}\")\n",
    "print(f\"Shape of training label: {example[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1751128668906,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "rWYWo--RYhPS",
    "outputId": "17d908c6-501c-40e7-821d-affcdfec4904"
   },
   "outputs": [],
   "source": [
    "forecast, forecast.shape, forecast.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1751128668910,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "OKSfFSx7lt46",
    "outputId": "37333776-72db-494e-caca-e97b4484dca5"
   },
   "outputs": [],
   "source": [
    "label = torch.rand(size=(1, 2, 1))\n",
    "label, label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1751128668944,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "RvVBWJWqn4MY",
    "outputId": "e038a746-eeee-4762-b9b8-e7643e57824c"
   },
   "outputs": [],
   "source": [
    "loss_fn(forecast, label.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1751128668978,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "xctWgzFuVHmd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Create training step function\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> float:\n",
    "  \"\"\"\n",
    "  Trains a PyTorch (regression) model for a single epoch.\n",
    "\n",
    "  Args:\n",
    "    model (torch.nn.Module): A PyTorch (regression) model to be trained.\n",
    "    dataloader (torch.utils.data.DataLoader): A DataLoader instance for the model to be trained on.\n",
    "    loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "    optimizer (torch.optim.Optimizer): A PyTorch optimizer to minimize the loss function.\n",
    "    device (torch.device): A target device to compute on (e.g. \"cuda\", \"mps\" or \"cpu\")\n",
    "  Returns:\n",
    "    (float) Average training loss across the epoch.\n",
    "  \"\"\"\n",
    "  # Put the model in train mode\n",
    "  model.train()\n",
    "\n",
    "  # Setup train loss\n",
    "  train_loss = 0\n",
    "\n",
    "  # Loop through data loader data batches\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Send data to target device\n",
    "    X, y = X.to(device), y.to(device) # downcast it to fp16 for timellm\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Calculate and accumulate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    # Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "  # Adjust metrics to get average loss per batch\n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  return train_loss\n",
    "\n",
    "# Create test step function\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> float:\n",
    "  \"\"\"\n",
    "  Tests a PyTorch (regression) model for a single epoch.\n",
    "\n",
    "  Args:\n",
    "    model (torch.nn.Module): A PyTorch (regression) model to be tested.\n",
    "    dataloader (torch.utils.data.DataLoader): A DataLoader instance for the model to be trained on.\n",
    "    loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "    device (torch.device): A target device to compute on (e.g. \"cuda\", \"mps\" or \"cpu\")\n",
    "\n",
    "  Returns:\n",
    "    (float) Average test loss across the epoch.\n",
    "  \"\"\"\n",
    "  # Put model in eval mode\n",
    "  model.eval()\n",
    "\n",
    "  # Setup test loss\n",
    "  test_loss = 0\n",
    "\n",
    "  # Turn on inference context manager\n",
    "  with torch.inference_mode():\n",
    "    # Loop through DataLoader batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "      # Send data to target device\n",
    "      X, y = X.to(device), y.to(device) # downcast it to fp16 for timellm\n",
    "\n",
    "      # Forward pass\n",
    "      y_pred = model(X)\n",
    "\n",
    "      # Calculate and accumulate loss\n",
    "      loss = loss_fn(y_pred, y)\n",
    "      test_loss += loss.item()\n",
    "\n",
    "  # Adjust metrics to get average loss\n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  return test_loss\n",
    "\n",
    "# Create a training function with train step and test step combined\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "  \"\"\"\n",
    "  Trains and tests a PyTorch model.\n",
    "\n",
    "  Returns:\n",
    "    (Dict) A dictionary of training and testing loss. Each metric has a value in a list for\n",
    "      each epoch.\n",
    "  \"\"\"\n",
    "  # Create empty results dictionary\n",
    "  results = {\n",
    "      \"train_loss\": [],\n",
    "      \"test_loss\": []\n",
    "  }\n",
    "\n",
    "  # Make sure model on target device\n",
    "  model.to(device)\n",
    "\n",
    "  # Loop through training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    train_loss = train_step(model=model,\n",
    "                            dataloader=train_dataloader,\n",
    "                            loss_fn=loss_fn,\n",
    "                            optimizer=optimizer,\n",
    "                            device=device)\n",
    "    test_loss = test_step(model=model,\n",
    "                          dataloader=test_dataloader,\n",
    "                          loss_fn=loss_fn,\n",
    "                          device=device)\n",
    "\n",
    "    # Print out what's happening\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1} | \"\n",
    "        f\"train_loss: {train_loss:.4f} | \"\n",
    "        f\"test_loss: {test_loss:.4f} | \"\n",
    "    )\n",
    "\n",
    "    # Update results dictionary\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "\n",
    "  # Return the filled results at the end of the epochs\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228,
     "referenced_widgets": [
      "f16c6e959433462bbcedd6d6560438f3",
      "67c860216e1e49b68e6dba9889fc1e6a",
      "f88a22a1269e44d0ae5d50b35c66c77a",
      "c4dedc3497bf4523a4696a754e69886f",
      "5ee9d405b0da474b81bfddaa69cb9ca7",
      "0ddad14d4b39442bb25fa63c696a85fc",
      "abe40b3d6c8e4153a80a812cc65c309f",
      "ce7ceac96f834a22bbce02e42044a3b3",
      "f1f7ba78436c4ab98846c79a676a3a06",
      "b08694e09b94499797acd40f48072585",
      "78664cfc85b84629b7e2ae3d167b7d4e"
     ]
    },
    "executionInfo": {
     "elapsed": 103281,
     "status": "ok",
     "timestamp": 1751128772260,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "eBtvCi3oPt2D",
    "outputId": "3021d314-f6e8-402c-c6fb-18d0ef2df962"
   },
   "outputs": [],
   "source": [
    "# Train our timellm model\n",
    "result = train(model=timellm,\n",
    "               train_dataloader=train_dataloader_custom,\n",
    "               test_dataloader=test_dataloader_custom,\n",
    "               optimizer=optimizer,\n",
    "               loss_fn=loss_fn,\n",
    "               epochs=10,\n",
    "               device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHkVesf6RkEU"
   },
   "source": [
    "## 6. Get predictions on test/custom data\n",
    "\n",
    "Now we've trained our TimeLLM model! It's time to predict on the test set/custom data.\n",
    "\n",
    "Let's create a function called `pred_and_plot` to make predictions and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1751128772336,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "s1w-bULoTdMw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "def pred_and_plot(model: torch.nn.Module,\n",
    "                  dataset: pd.DataFrame,\n",
    "                  columns: Optional[List[str]] = None,\n",
    "                  batch_size: int = 32,\n",
    "                  window_size: int = 24,\n",
    "                  forecast_horizon: int = 1,\n",
    "                  stride: int = 1,\n",
    "                  device: torch.device = DEVICE) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "  \"\"\"\n",
    "  Run a trained PyTorch (regression) model of time series and plot predictions vs. ground truth.\n",
    "\n",
    "  Args:\n",
    "    model (torch.nn.Module): A trained PyTorch time series forecasting model.\n",
    "    dataset (pd.DataFrame): The target DataFrame you want to run inference on.\n",
    "    columns (Optional[List[str]]): Target columns included in the plot.\n",
    "    batch_size (int): Number of samples in each batch.\n",
    "    window_size (int): Number of time steps included in each time series window.\n",
    "    forecast_horizon (int): Number of time steps into the future you want the model to predict.\n",
    "    stride (int): Step size for the sliding window over the time series.\n",
    "    device (torch.device): A target device to run inference on (e.g. \"cuda\", \"mps\" or \"cpu\")\n",
    "\n",
    "  Returns:\n",
    "    (Tuple[torch.Tensor, torch.Tensor]) A tuple of ground truth labels and model predictions\n",
    "      in the form of tensor. They are of shape (`num_channels`, `len_pred`), where `num_channels` is\n",
    "      the number of time series in the dataset and `len_pred` is the length of model predictions.\n",
    "  \"\"\"\n",
    "  # Create empty lists to store labels and predictions\n",
    "  labels = []\n",
    "  predictions = []\n",
    "\n",
    "  # Calculate number of workers\n",
    "  num_workers = os.cpu_count()\n",
    "\n",
    "  # Turn the raw Dataframe into an instance of TimeSeriesDatasetCustom\n",
    "  custom_dataset = TimeSeriesDatasetCustom(df=dataset,\n",
    "                                           window_size=window_size,\n",
    "                                           forecast_horizon=forecast_horizon,\n",
    "                                           stride=stride)\n",
    "\n",
    "  # Turn the custom pytorch Datasets into DataLoaders\n",
    "  dataloader = DataLoader(dataset=custom_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False) # don't drop the last batch\n",
    "\n",
    "  # Make sure the model is on the target device\n",
    "  model.to(device)\n",
    "\n",
    "  # Put model in eval mode\n",
    "  model.eval()\n",
    "\n",
    "  # Turn on inference manager\n",
    "  with torch.inference_mode():\n",
    "    # Loop through DataLoader batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "      # Send data to target device\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      y_pred = model(X)\n",
    "\n",
    "      # Append the label to a list\n",
    "      labels.append(y)\n",
    "\n",
    "      # Append the prediction to a list\n",
    "      predictions.append(y_pred)\n",
    "\n",
    "  # Concatenate predictions along the batch dimension\n",
    "  all_labels = torch.cat(labels, dim=0)\n",
    "  all_preds = torch.cat(predictions, dim=0)\n",
    "\n",
    "  # Permute the batch size and number of channels and then flatten them -> (num_channels, batch_size*forecast_horizon)\n",
    "  all_labels = all_labels.permute(1, 0, 2).flatten(start_dim=1)\n",
    "  all_preds = all_preds.permute(1, 0, 2).flatten(start_dim=1)\n",
    "\n",
    "  # Plot the graph\n",
    "  # Check if any columns are selected, if not, defaults to all columns\n",
    "  if columns is None:\n",
    "    columns = list(dataset.columns)\n",
    "\n",
    "  # Get the index of the selected columns in the dataframe column list\n",
    "  column_index_list = [list(dataset.columns).index(column) for column in columns]\n",
    "\n",
    "  # Create column names list for the labels and predictions dataframe\n",
    "  column_names = columns + [f\"{column}_pred\" for column in columns]\n",
    "\n",
    "  # Select the specificed columns' labels and predictions\n",
    "  selected_labels = all_labels[column_index_list, :]\n",
    "  selected_preds = all_preds[column_index_list, :]\n",
    "\n",
    "  # Concatenate the labels and predictions along the columns\n",
    "  label_and_pred_tensor = torch.cat([selected_labels.transpose(0, 1), selected_preds.transpose(0, 1)], dim=1)\n",
    "\n",
    "  # Create a DataFrame to store the labels and predictions\n",
    "  label_and_pred_df = pd.DataFrame(data=label_and_pred_tensor.cpu(),\n",
    "                                   columns=column_names) # need to move the tensor to cpu because matplotlib works with numpy arrays\n",
    "\n",
    "  # Plot the labels and predictions\n",
    "  label_and_pred_df.plot();\n",
    "\n",
    "  return all_labels, all_preds # each of them has shape: (num_channels, len_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 5267,
     "status": "ok",
     "timestamp": 1751128777604,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "mPHx29l28Ske",
    "outputId": "fbaf88d4-3d7d-4bb2-fe5e-45824847d569"
   },
   "outputs": [],
   "source": [
    "# Plot training data and predictions\n",
    "train_labels_and_preds = pred_and_plot(model=timellm,\n",
    "                                       dataset=train_df)\n",
    "train_labels, train_preds = train_labels_and_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1751128779134,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "mwWpEh6FqvZ8",
    "outputId": "194e7060-9b11-4cc8-8504-aa85df63ac34"
   },
   "outputs": [],
   "source": [
    "# Plot test data and predictions\n",
    "test_label_and_preds = pred_and_plot(model=timellm,\n",
    "                                     dataset=test_df)\n",
    "test_label, test_preds = test_label_and_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9v3f8-pyir-"
   },
   "source": [
    "## 7. Prompt-as-Prefix Ablation\n",
    "\n",
    "We've just trained our first timellm model! Let's do a quick ablation (with and without prompt as prefix) to study the impact of the prompt on the model's predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjbOStZd0Fsx"
   },
   "source": [
    "### 7.1 Create a new instance of TimeLLM model without prompt as prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1751128779882,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "meXNPRDd0QOT"
   },
   "outputs": [],
   "source": [
    "# Create an instance of TimeLLM without prompt\n",
    "timellm_without_prompt = TimeLLM(checkpoint=\"distilgpt2\",\n",
    "                                 num_channels=2,\n",
    "                                 window_size=24,\n",
    "                                 forecast_horizon=1,\n",
    "                                 patch_size=2,\n",
    "                                 stride=2,\n",
    "                                 torch_dtype=torch.float32,\n",
    "                                 vocab_size=50257,\n",
    "                                 hidden_size=768,\n",
    "                                 prompt_as_prefix=None).to(DEVICE) # send the model to gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tT6GE-wdzky-"
   },
   "source": [
    "### 7.2 Setup optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1751128779884,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "u3p-hhpEztP8"
   },
   "outputs": [],
   "source": [
    "# Setup optimizer and loss function\n",
    "optimizer = torch.optim.Adam(params=timellm_without_prompt.parameters(),\n",
    "                             lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryqXvRrG02-b"
   },
   "source": [
    "### 7.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228,
     "referenced_widgets": [
      "a8d0502c10334fc9862c9062f605276c",
      "14858de6d196460bbd245989ec7b1de0",
      "7ed47207dd3d45788f130ddf26f60d5e",
      "b152b0026e1144249d391ddf63d7abc6",
      "9c8052ab94aa458fa7552be6da0dde21",
      "49d0b32447034a23b75c8290419c17bf",
      "24c96bcf840547c2871076cdc520825f",
      "d61cdb2894344b77a78eaed8542d4293",
      "1f17403c9ff94104aee7702216305d56",
      "032720f90845459691acc227e9d2bddb",
      "be444e81e2104bb0ae0f8dd82d73f0d9"
     ]
    },
    "executionInfo": {
     "elapsed": 51034,
     "status": "ok",
     "timestamp": 1751128830919,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "7QQ3qO-U05vS",
    "outputId": "95b6ef99-1f04-422f-e1e2-e0409a40c1a5"
   },
   "outputs": [],
   "source": [
    "# Train our timellm model\n",
    "result_without_prompt = train(model=timellm_without_prompt,\n",
    "                              train_dataloader=train_dataloader_custom,\n",
    "                              test_dataloader=test_dataloader_custom,\n",
    "                              optimizer=optimizer,\n",
    "                              loss_fn=loss_fn,\n",
    "                              epochs=10,\n",
    "                              device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 3415,
     "status": "ok",
     "timestamp": 1751128834343,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "Rj8IB4QB2Hdh",
    "outputId": "fe937fa7-9c7d-4300-d738-991cab9cbc09"
   },
   "outputs": [],
   "source": [
    "# Plot training data and predictions\n",
    "train_labels_and_preds = pred_and_plot(model=timellm_without_prompt,\n",
    "                                       dataset=train_df)\n",
    "train_labels, train_preds = train_labels_and_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1751128835499,
     "user": {
      "displayName": "Edison Fung",
      "userId": "07922641752347202748"
     },
     "user_tz": -120
    },
    "id": "U6DCLbq82KgH",
    "outputId": "02406ad5-fd3a-4aae-afe9-56b93141825a"
   },
   "outputs": [],
   "source": [
    "# Plot test data and predictions\n",
    "test_label_and_preds = pred_and_plot(model=timellm_without_prompt,\n",
    "                                     dataset=test_df)\n",
    "test_label, test_preds = test_label_and_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqfWunnP2tZJ"
   },
   "source": [
    "## 8. Comparing the results\n",
    "\n",
    "Let's compare the ablation results (with and without prompt):\n",
    "* Convergence speed: The prompt-augmented model converges much faster. The no-prompt model reaches its best test loss of 0.0507 by epoch 3, whereas the prompt-augmented model reaches a similar level (even lower) of test loss (0.0502) by epoch 3.\n",
    "* Forecast Accuracy: The prompt-augmented model also gives more robust forecasts. Its minimum test loss outperforms the no-prompt model (0.0481 vs 0.0502). This suggests the information in the prompt, e.g. telling the model about underlying DGP's periodicity and noise distribution, instructing the model to focus on learning the dominant periodic patterns, helps improves the predictions. Whereas the no-prompt model seems to overfit the noise in the training data, we can tell by the difference in the smoothness of the prediction curves.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOfv6eWM6lPmOXYhIvY+PLm",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1fJhg4TwODj0pDsrSAf0BVfCWzSqBhESZ",
     "timestamp": 1750642536362
    },
    {
     "file_id": "16frMoOCg6--NLM83Ti4G3ZpLJ1nPSz0q",
     "timestamp": 1749173766068
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "032720f90845459691acc227e9d2bddb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b6cc08114e54699bacb07ba22916760": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ba4c8dfb41e42b3a2c7089f3a682459": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ae1513f19a74f158a9618a022cc1271",
       "IPY_MODEL_9f97dea8485841ca8807d54c94bfa7a2",
       "IPY_MODEL_3de485a06c1c4f4baf9a670c97adef56"
      ],
      "layout": "IPY_MODEL_f3a836447b5b4ceca79dea21a8c015b7"
     }
    },
    "0ddad14d4b39442bb25fa63c696a85fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fdfadfe6891437eab6cbf4b17ff97ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1022228c36174c238b48c3cddd7d685f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11bea1d5532d401981608d77cb152991": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11c7e3d5ae5a4af98859c98848c5e497": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12033af79f5f4fc5bf58186926f03e81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4560aa9a160e444aa6fee952f7e4e0a9",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_23019d8ed0554ebb93e077353c7437ed",
      "value": 1042301
     }
    },
    "127f6d5f72e1477dbbe01ff480fe9e22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c255a43be62d4b9ba145e764e060a950",
       "IPY_MODEL_12033af79f5f4fc5bf58186926f03e81",
       "IPY_MODEL_89c320ae30e548e6afe20e1fe919f7da"
      ],
      "layout": "IPY_MODEL_11c7e3d5ae5a4af98859c98848c5e497"
     }
    },
    "14858de6d196460bbd245989ec7b1de0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49d0b32447034a23b75c8290419c17bf",
      "placeholder": "​",
      "style": "IPY_MODEL_24c96bcf840547c2871076cdc520825f",
      "value": "100%"
     }
    },
    "14be3655a8354b4196105eaec09a7f4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_634dc4558b7c4b409d673552c0090e64",
      "placeholder": "​",
      "style": "IPY_MODEL_6d8f26c98e8c4841bfcd381abb68c5b5",
      "value": "merges.txt: 100%"
     }
    },
    "1c57b3b795354e029670864ebf6488e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8097e73fbca441629f564666b219987a",
      "placeholder": "​",
      "style": "IPY_MODEL_e231d5bf5f50462db44bf24a007b7d88",
      "value": " 26.0/26.0 [00:00&lt;00:00, 2.82kB/s]"
     }
    },
    "1ce181f258cc47e29eb64b8ccb3de794": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d431a96af7743b2bfe87df92dc14a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e31deefd9b24abf994a759c77ddc5b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98e2dd4f64d34419870210bc4413363a",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_762639f6009542a2806081c94d7de200",
      "value": 26
     }
    },
    "1f17403c9ff94104aee7702216305d56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2199365465ba4fe895272418fe9b37a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23019d8ed0554ebb93e077353c7437ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "24c96bcf840547c2871076cdc520825f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27ec0f7a9d244db5befa33401a02943a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "290bd5ff6132455abdbf1fbb47de4350": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a66cdc322344b81820749e4bd9fe497": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32782a84bdb741218aa607fc67634da7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "32a5bef958ef436798264c28b13bec98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fb5aa9657714d19b18a5c0ab3d93478",
      "placeholder": "​",
      "style": "IPY_MODEL_86ab795d2a57485ab0a4e2a6792b0836",
      "value": " 762/762 [00:00&lt;00:00, 65.8kB/s]"
     }
    },
    "3430064a87c8447394c3a624097d4df2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_beebd466ef78475196d8107d8cd08be6",
      "placeholder": "​",
      "style": "IPY_MODEL_2199365465ba4fe895272418fe9b37a0",
      "value": "tokenizer.json: 100%"
     }
    },
    "367f3242d4b0415fa604773d7d1b2622": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a0c7315287448f991ae7ce5aa150942": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3de485a06c1c4f4baf9a670c97adef56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fdfadfe6891437eab6cbf4b17ff97ca",
      "placeholder": "​",
      "style": "IPY_MODEL_492f47725fb24915a37f642f97f0b836",
      "value": " 353M/353M [00:01&lt;00:00, 327MB/s]"
     }
    },
    "4215153bfc46411e83c804334ac9d5d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43233ce5e7da424fa1e17589292ea2b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4560aa9a160e444aa6fee952f7e4e0a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "470063248956448c9ddb684c3be40492": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_794e83002a19418e9bb30e4f69cdea01",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4fcfafb2e744a31be57380769fdd68e",
      "value": 456318
     }
    },
    "492f47725fb24915a37f642f97f0b836": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49d0b32447034a23b75c8290419c17bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4eb398534fb947a5ba0ac3e1ea73435a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ee9d405b0da474b81bfddaa69cb9ca7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "634dc4558b7c4b409d673552c0090e64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6471fb92e98a4d2aaa17b81c852bb441": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4eb398534fb947a5ba0ac3e1ea73435a",
      "placeholder": "​",
      "style": "IPY_MODEL_f2058e6e350b40edadfab315544ef0b4",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "67c860216e1e49b68e6dba9889fc1e6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ddad14d4b39442bb25fa63c696a85fc",
      "placeholder": "​",
      "style": "IPY_MODEL_abe40b3d6c8e4153a80a812cc65c309f",
      "value": "100%"
     }
    },
    "6a4baa6afbe546fc998ac31f3b25dacd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d8f26c98e8c4841bfcd381abb68c5b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "762639f6009542a2806081c94d7de200": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "78664cfc85b84629b7e2ae3d167b7d4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "794e83002a19418e9bb30e4f69cdea01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ae1513f19a74f158a9618a022cc1271": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e57099784374980b832fffd1c5ff992",
      "placeholder": "​",
      "style": "IPY_MODEL_2a66cdc322344b81820749e4bd9fe497",
      "value": "model.safetensors: 100%"
     }
    },
    "7d129bcb99e74716b4e5fcfb6323ceab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ed47207dd3d45788f130ddf26f60d5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d61cdb2894344b77a78eaed8542d4293",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1f17403c9ff94104aee7702216305d56",
      "value": 10
     }
    },
    "8097e73fbca441629f564666b219987a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86ab795d2a57485ab0a4e2a6792b0836": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8872ffec5de5448ca881755f604c6c31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d129bcb99e74716b4e5fcfb6323ceab",
      "placeholder": "​",
      "style": "IPY_MODEL_1d431a96af7743b2bfe87df92dc14a3b",
      "value": " 456k/456k [00:00&lt;00:00, 6.94MB/s]"
     }
    },
    "89c320ae30e548e6afe20e1fe919f7da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_367f3242d4b0415fa604773d7d1b2622",
      "placeholder": "​",
      "style": "IPY_MODEL_1022228c36174c238b48c3cddd7d685f",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 5.23MB/s]"
     }
    },
    "8ac7c5155c6b403b9a46434304996578": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "926b51c9313c4b67bb124b784f2cf470": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98e2dd4f64d34419870210bc4413363a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c8052ab94aa458fa7552be6da0dde21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d7af3add0984a36b2478c87c8adb7df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9e57099784374980b832fffd1c5ff992": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f0a3a6079af440e942bed23a17c7930": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14be3655a8354b4196105eaec09a7f4c",
       "IPY_MODEL_470063248956448c9ddb684c3be40492",
       "IPY_MODEL_8872ffec5de5448ca881755f604c6c31"
      ],
      "layout": "IPY_MODEL_290bd5ff6132455abdbf1fbb47de4350"
     }
    },
    "9f97dea8485841ca8807d54c94bfa7a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11bea1d5532d401981608d77cb152991",
      "max": 352824413,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32782a84bdb741218aa607fc67634da7",
      "value": 352824413
     }
    },
    "9fb5aa9657714d19b18a5c0ab3d93478": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5e31316eab44e278d62393c0f104e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6471fb92e98a4d2aaa17b81c852bb441",
       "IPY_MODEL_1e31deefd9b24abf994a759c77ddc5b4",
       "IPY_MODEL_1c57b3b795354e029670864ebf6488e4"
      ],
      "layout": "IPY_MODEL_43233ce5e7da424fa1e17589292ea2b2"
     }
    },
    "a8d0502c10334fc9862c9062f605276c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14858de6d196460bbd245989ec7b1de0",
       "IPY_MODEL_7ed47207dd3d45788f130ddf26f60d5e",
       "IPY_MODEL_b152b0026e1144249d391ddf63d7abc6"
      ],
      "layout": "IPY_MODEL_9c8052ab94aa458fa7552be6da0dde21"
     }
    },
    "ab9514081130454988271b26ba799bed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abe40b3d6c8e4153a80a812cc65c309f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "adae7f4803654e96b76d0eed8ea041c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b08694e09b94499797acd40f48072585": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b152b0026e1144249d391ddf63d7abc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_032720f90845459691acc227e9d2bddb",
      "placeholder": "​",
      "style": "IPY_MODEL_be444e81e2104bb0ae0f8dd82d73f0d9",
      "value": " 10/10 [00:51&lt;00:00,  5.10s/it]"
     }
    },
    "be444e81e2104bb0ae0f8dd82d73f0d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "beebd466ef78475196d8107d8cd08be6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfbda13351524fd097ff94991a125209": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe54249c1b8241e4ac8a5064a53d6edb",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ac7c5155c6b403b9a46434304996578",
      "value": 1355256
     }
    },
    "c255a43be62d4b9ba145e764e060a950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab9514081130454988271b26ba799bed",
      "placeholder": "​",
      "style": "IPY_MODEL_3a0c7315287448f991ae7ce5aa150942",
      "value": "vocab.json: 100%"
     }
    },
    "c4dedc3497bf4523a4696a754e69886f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b08694e09b94499797acd40f48072585",
      "placeholder": "​",
      "style": "IPY_MODEL_78664cfc85b84629b7e2ae3d167b7d4e",
      "value": " 10/10 [01:43&lt;00:00, 10.30s/it]"
     }
    },
    "c8e79a78e16f4b2a9e8c58e2d30781a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de080f8d514640aab913d45cac15d3cc",
       "IPY_MODEL_f328a19310884a02848d6777b80cea4d",
       "IPY_MODEL_32a5bef958ef436798264c28b13bec98"
      ],
      "layout": "IPY_MODEL_4215153bfc46411e83c804334ac9d5d2"
     }
    },
    "ce7ceac96f834a22bbce02e42044a3b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d61cdb2894344b77a78eaed8542d4293": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de080f8d514640aab913d45cac15d3cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27ec0f7a9d244db5befa33401a02943a",
      "placeholder": "​",
      "style": "IPY_MODEL_6a4baa6afbe546fc998ac31f3b25dacd",
      "value": "config.json: 100%"
     }
    },
    "e231d5bf5f50462db44bf24a007b7d88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4fcfafb2e744a31be57380769fdd68e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f16c6e959433462bbcedd6d6560438f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_67c860216e1e49b68e6dba9889fc1e6a",
       "IPY_MODEL_f88a22a1269e44d0ae5d50b35c66c77a",
       "IPY_MODEL_c4dedc3497bf4523a4696a754e69886f"
      ],
      "layout": "IPY_MODEL_5ee9d405b0da474b81bfddaa69cb9ca7"
     }
    },
    "f1f7ba78436c4ab98846c79a676a3a06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2058e6e350b40edadfab315544ef0b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f328a19310884a02848d6777b80cea4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ce181f258cc47e29eb64b8ccb3de794",
      "max": 762,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d7af3add0984a36b2478c87c8adb7df",
      "value": 762
     }
    },
    "f3a836447b5b4ceca79dea21a8c015b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f868b62f00e741a28f0616595cbfe8b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3430064a87c8447394c3a624097d4df2",
       "IPY_MODEL_bfbda13351524fd097ff94991a125209",
       "IPY_MODEL_fb6fa72b087a4202ad189d45ffbb5a70"
      ],
      "layout": "IPY_MODEL_0b6cc08114e54699bacb07ba22916760"
     }
    },
    "f88a22a1269e44d0ae5d50b35c66c77a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce7ceac96f834a22bbce02e42044a3b3",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1f7ba78436c4ab98846c79a676a3a06",
      "value": 10
     }
    },
    "fb6fa72b087a4202ad189d45ffbb5a70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adae7f4803654e96b76d0eed8ea041c3",
      "placeholder": "​",
      "style": "IPY_MODEL_926b51c9313c4b67bb124b784f2cf470",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 9.38MB/s]"
     }
    },
    "fe54249c1b8241e4ac8a5064a53d6edb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
